# Mask R-CNN

### Abstract

1. 我们提出了一个概念简单、灵活和通用的对象实例分割框架。
2. 我们的方法有效地检测图像中的对象，同时为每个实例生成高质量的分割掩码。
3. 该方法称为Mask R-CNN，通过添加一个分支来预测一个对象掩码，与现有的用于边界框识别的分支并行，从而扩展了 Faster R-CNN。
4. Mask R-CNN训练简单，只增加了一个小开销到更快的R-CNN，运行在5帧每秒。
5. 此外， Mask R-CNN很容易推广到其他任务，例如，允许我们在相同的框架下估计人类的姿态。
6. 我们展示了COCO套件中所有三个方面的顶级结果，包括实例分割、边界框对象检测和人员关键点检测。
7. 在没有技巧的情况下，Mask R-CNN在每个任务上的表现都超过了所有现有的单模型条目，包括COCO 2016挑战赛冠军。
8. 我们希望我们的简单而有效的方法将作为一个坚实的基线，并有助于简化未来在实例级识别方面的研究。将提供代码。

## Introduction

1. 视觉社区在短时间内快速改进了目标检测和语义分割结果。
2. 在很大程度上，这些进步是由强大的基线系统驱动的，比如分别用于对象检测和语义分割的Fast/Faster RCNN[9,29]和full Convolutional Network (FCN)[24]框架。
3. 这些方法在概念上是直观的，具有灵活性和鲁棒性，以及快速的训练和推理时间。
4. 我们在这项工作中的目标是开发一个支持实例分割的框架
5. 实例分割具有挑战性，因为它要求正确地检测图像中的所有对象，同时精确地分割每个实例。
6. 因此，它结合了经典的计算机视觉任务中的元素:目标是对单个对象进行分类，并使用一个边界框对每个对象进行本地化;目标是将每个像素分类为一组固定的类别，而不区分对象实例。
7. 考虑到这一点，人们可能认为需要使用复杂的方法才能获得良好的结果
8. 然而，我们展示了一个令人惊讶的简单、灵活和快速的系统可以超越以前最先进的实例分割结果。
9. 我们的方法称为Mask R-CNN，通过添加一个分支来预测感兴趣的每个区域(RoI)的分割掩码，从而扩展了Faster R-CNN[29]，与现有的用于分类和边界框回归的分支并行(图1)。
10. 掩码分支是一个应用于每个RoI的小FCN，以像素对像素的方式预测分割掩码。
11. Mask R-CNN是简单的实现和培训，因为更快的R-CNN框架，这有利于广泛的灵活的架构设计。
12. 此外，掩码分支只增加了很小的计算开销，支持快速系统和快速实验
13. 原则上，Mask R-CNN是Faster R-CNN的一个直观扩展，但是正确地构造掩码分支对于获得良好的结果是至关重要的
14. 最重要的是，更快的R-CNN不是为网络输入和输出之间的像素对像素对齐而设计的。
15. 这一点在RoIPool[14,9]中表现得最为明显，RoIPool实际上是处理实例的核心操作，它执行粗空间量化来提取特征。
16. 为了修正这个偏差，我们提出了一个简单的、无量化的层，称为RoIAlign，它忠实地保留了精确的空间位置。
17. 尽管看起来是一个很小的变化，**RoIAlign**却有很大的影响:它将掩码精度提高了10%到50%，在更严格的本地化指标下显示出更大的收益。
18. 其次，我们发现解耦掩码和类预测是必要的:我们独立地为每个类预测一个二进制掩码，不存在类之间的竞争，并依赖于网络的RoI分类分支来预测类别。
19. 而FCNs通常采用逐像素多类分类的方法，将分割和分类结合起来，基于我们的实验，在实例分割方面效果不佳。
20. 没有什么花里胡哨的东西，Mask R-CNN超过了之前所有的最先进的单模型在可可实例分割任务[23]的结果，包括从2016年大赛冠军的繁重工程的条目。
21. 作为一个副产品，我们的方法也擅长于COCO对象检测任务。
22. 在烧蚀实验中，我们评估了多个基本实例，这使我们能够证明其鲁棒性，并分析核心因素的影响。
23. 我们的模型可以在GPU上以每帧200毫秒的速度运行，在一台8-GPU的机器上进行COCO的训练需要一到两天的时间。
24. 我们相信，快速的训练和测试速度，以及框架的灵活性和准确性，将有利于和简化未来的研究实例分割。
25. 最后，通过对COCO keypoint数据集[23]进行人体姿态估计，展示了该框架的通用性。
26. 通过将每个关键点看作一个单一的热二进制掩码，用最小的修改Mask R-CNN可以检测特定于实例的姿态
27. 没有技巧，Mask R-CNN超越了2016年COCO keypoint大赛冠军，同时以5帧的速度运行
28. 因此，Mask R-CNN可以被广泛地看作是一个用于实例级识别的灵活框架，并且可以很容易地扩展到更复杂的任务中。
29. 我们将发布代码以方便将来的研究

## Related Word

1. **R-CNN**:基于区域的CNN (R-CNN)的边界盒目标检测方法[10]是关注一个可管理的候选对象区域数[33,16]，并在每个RoI上独立评估卷积网络[20,19]。
2. R-CNN得到了扩展[14,9]，允许使用RoIPool处理特征地图上的roi，从而提高了速度和准确性。
3. 更快的R-CNN[29]通过学习区域建议网络(RPN)的注意机制，改进了这一流程
4. 更快的R-CNN对于许多后续的改进(例如，[30,22,17])是灵活和健壮的，并且是当前在几个基准测试中领先的框架。
5. **Instance Segmentation**:在RCNN有效性的驱动下，许多实例分割方法都是基于分段建议的。
6. 较早的方法[10,12,13,6]采用自底向上分段[33,2]。
7. 深度掩码[27]及后续工作[28,5]学习提出分段候选，然后快速R-CNN对其进行分类。
8. 在这些方法中，分割先于识别，识别速度慢，精度低
9. 同样地，Dai等人提出了一个复杂的多级级联方法，该方法从边界框建议中预测分段建议，然后进行分类。
10. 相反，我们的方法是基于掩码和类标签的并行预测，这更简单、更灵活。
11. 最近，Li等人将[5]中的segment proposal system和[8]中的object detection system结合起来，实现了“全卷积实例分割”(full convolutional instance segmentation, FCIS)。
12. 在[5,8,21]中，通常的想法是完全卷积地预测一组位置敏感的输出通道
13. 这些通道同时处理对象类、框和掩码，使系统速度更快。
14. 但是FCIS在重叠实例上显示出系统性错误，并创建假边(图5)，这表明它受到了实例分段的基本困难的挑战

## Mask R-CNN

1. Mask R-CNN在概念上很简单:更快的R-CNN对每个候选对象有两个输出，一个类标签和一个边界框偏移量;为此，我们添加了第三个分支，输出对象Mask。

2. Mask R-CNN因此是一个自然和直观的想法。

3. 但是额外的掩码输出与类和框输出不同，需要提取对象的更精细的空间布局

4. 接下来，我们介绍了Mask R-CNN的关键元素，包括像素对像素的对齐，这是Fast/Faster R-CNN的主要缺失部分

5. **Faster R-CNN**:我们首先简要回顾一下更快的R-CNN检测器

6. Faster R-CNN包括两个阶段。

7. 第一阶段称为区域建议网络(RPN)，提出候选对象边界框

8. 第二阶段本质上是Faster R-CNN[9]，利用RoIPool从每个候选框中提取特征，进行分类和边界框回归

9. 这两个阶段使用的特性可以共享，以便更快地进行推理

10. 我们向读者推荐[17]，以获得更快的R-CNN和其他框架之间最新的、全面的比较。

11. **Mask R-CNN**: Mask R-CNN采用相同的两阶段过程，第一阶段相同(即RPN)。

12. 在第二阶段，除了预测类和框偏移量，Mask R-CNN还为每个RoI输出一个二进制掩码。

13. 这与最近的系统形成了对比，在最近的系统中，分类依赖于掩码预测(例如[27,7,21])。

14. 我们的方法遵循Faster R-CNN[9]的精神，并行地应用了边界框分类和回归(这在很大程度上简化了原始R-CNN[10]的多级管道)。

15. 在训练过程中，我们将每个采样RoI上的多任务损失正式定义为
    $$
    L = L_{cls} + L_{box} + L_{mask}
    $$

16. 分类损失Lcls和边界盒损失Lbox与[9]中定义的分类损失Lcls和边界盒损失Lbox相同。

17. Mask分支对于每个RoI有$km^2$ 维的输出，它编码K个分辨率为m×m的二进制mask，K个类中每个都对应一个

18. 为此，我们应用了一个逐像素的sigmoid，并将$L_{mask}$定义为平均二进制交叉熵损失

19. 对于与ground-truth类k相关联的RoI, $L_{mask}$只定义在第k个掩码上(其他掩码输出不会造成损失)。

20. 我们对$L_{mask}$的定义允许网络为每个类生成掩码，而不存在类之间的竞争，我们依赖于专用的分类分支来预测用于选择输出掩码的类标签

21. 这个解耦掩码和类预测

22. 这与将FCNs[24]应用于语义分割的常见做法不同，语义分割通常使用每个像素的softmax和多项交叉熵损失。

23. 在这种情况下，跨类的掩码竞争;在我们的例子中，对于每个像素的sigmoid和二进制损失，它们没有。

24. 实验表明，该方法是实现实例分割效果的关键。

25. **Mask Representation**:掩码编码输入对象的空间布局。

26. 因此，与类标签或盒偏移量不可避免地被全连接(fc)层折叠成短输出向量不同，提取掩码的空间结构可以通过卷积提供的像素对像素的对应关系自然地进行处理。

27. 具体来说，我们使用FCN[24]从每个RoI中预测一个m×m掩码。

28. 这允许掩模分支中的每一层维护显式的m×m对象空间布局，而不会将其折叠成缺乏空间维度的向量表示形式

29. 与以往使用fc层进行掩模预测的方法[27,28,7]不同，我们的全卷积表示需要更少的参数，而且实验证明更准确

30. 这种像素对像素的行为要求我们的RoI特性(它们本身就是小的特征映射)保持良好的对齐，以忠实地保存显式的每个像素的空间对应关系。

31. 这促使我们开发下面的RoIAlign层，它在掩模预测中起着关键作用

32. **RoIAlign**:RoIPool[9]是从每个RoI中提取小feature map(例如，7×7)的标准操作

33. RoIPool首先将一个浮点数RoI量化为特征图的离散粒度，然后将这个量化的RoI细分为空间bin，空间bin本身也进行了量化，最后将每个bin覆盖的特征值进行聚合(通常通过max pooling)。

34. 通过计算[x/16]对连续坐标x进行量化，其中16为feature map stride，[·]为舍入;同样的，量化是在分成箱子的时候进行的(例如，7×7)。

35. 这些量化在RoI和提取的特征之间引入了misalignments。

36. 为了解决这个问题，我们提出了一个RoIAlign层，它消除了RoIPool的严格量化，正确地将提取的特性与输入对齐。

37. 我们建议的改变很简单:我们避免任何RoI边界或箱子的量化(即，我们用x/16代替[x/16])。

38. 我们使用双线性插值[18]来计算每个RoI bin中四个定期采样位置的输入特征的精确值，并将结果聚合(使用max或average).2

39. 正如我们在§4.2中所展示的，RoIAlign带来了很大的改进。

40. 我们还比较了在[7]中提出的RoIWarp操作。

41. 与RoIAlign不同，RoIWarp忽略了对齐问题，并像RoIPool一样在[7]中实现了RoI的量化。

42. 因此，虽然RoIWarp也采用了[18]激励下的双线性重采样，但实验表明其性能与RoIPool相当(详见表2c)，说明了对齐的关键作用

43. **Network Architecture**:为了演示我们的方法的通用性，我们用多个架构实例化了Mask R-CNN

44. 为了清晰起见，我们区分了:(i)用于整个图像特征提取的卷积主干架构，以及(ii)分别应用于每个RoI的边界框识别(分类和回归)和掩码预测的网络头。

45. 我们使用命名法网络深度特性来表示主干架构。

46. 我们评估了深度为50或101层的ResNet[15]和ResNeXt[35]网络

47. 原始实现的Faster R-CNN与ResNets[15]提取的特征，从最后的卷积层的第4阶段，我们称之为C4

48. 例如，这个带有ResNet-50的主干用ResNet-50- c4表示。

49. 这是[15,7,17,31]中常用的选择。

50. 我们还探索了Lin等人最近提出的另一种更有效的骨干，称为特征金字塔网络(FPN)。

51. FPN采用自顶向下的横向连接架构，从单尺度输入构建网络内特征金字塔。

52. Faster R-CNN与FPN骨干提取RoI特征从不同层次的特征金字塔根据他们的规模，但其他方面的方法类似于vanilla ResNet

53. 使用ResNet-FPN骨干网与Mask RCNN进行特征提取，在精度和速度上都有很好的提高。

54. 有关FPN的详细信息，请参考[22]。

55. 对于网络头，我们密切关注前面工作中介绍的架构，其中我们添加了一个完全卷积掩码预测分支

56. 具体来说，我们从ResNet[15]和FPN[22]论文中扩展了Faster R-CNN盒头。

57. 细节如图3所示。

58. 在ResNet- c4主干上的头部包括ResNet的第5阶段(即9层的“res5”[15])，这是计算密集型的。

59. 对于FPN，主干已经包含res5，因此允许使用更少过滤器的更有效的head。

60. 我们注意到掩码分支有一个简单的结构。

61. 更复杂的设计有潜力提高性能，但不是这项工作的重点。

### Implementation Details

1. 我们根据现有的快速/更快的R-CNN工作来设置超参数[9,29,22]。
2. 虽然这些决策是在原始文献[9,29,22]中针对对象检测做出的，但是我们发现我们的实例分割系统对这些决策是鲁棒的。
3. **Training**:就像在Fast R-CNN中一样，如果RoI具有IoU，且ground-truth框至少为0.5，则认为RoI为正，否则为负。
4. mask loss Lmask仅在positive roi上定义。
5. 掩码目标是RoI与其相关的地面真相掩码之间的交集。
6. 我们采用以图像为中心的训练[9]。
7. 图像调整大小，使其比例(较短的边缘)为800像素[22]。
8. 每个小批处理每个GPU有2张图像，每个图像有N个采样roi，正误比为1:3[9]。
9. C4主干的N为64(如[9,29])，FPN为512(如[22])。
10. 我们使用8个gpu(因此有效的小批量大小为16)进行16万次迭代的训练，学习速度为0.02，在12万次迭代时学习速度降低了10。
11. 我们使用重量衰减为0.0001，动量为0.9。
12. 根据[22]，RPN锚跨5个尺度和3个纵横比。
13. 为了方便消融，RPN是单独训练的，不与mask R-CNN共享功能，除非指定
14. 对于本文中的每个条目，RPN和Mask R-CNN都有相同的主干，因此它们是可共享的
15. **Inference**:在测试时，提案号为C4主干300(如[29])和FPN 1000(如[22])。
16. 我们对这些建议运行box预测分支，然后进行非最大抑制
17. 然后将掩模分支应用于得分最高的100个检测框
18. 虽然这与训练中使用的并行计算不同，但它加快了推理速度并提高了准确性(由于使用更少、更准确的roi)。
19. 掩码分支可以预测每个RoI的K个掩码，但是我们只使用第K个掩码，其中K是分类分支预测的类
20. 然后将m×m浮点数掩码输出调整为RoI大小，并以0.5为阈值进行二值化。
21. 注意，由于我们只计算前100个检测框上的掩码，掩码R-CNN为其更快的对应的R-CNN增加了一小部分开销(例如，在典型模型上增加20%)。

### Experiments: Instance Segmentation

1. 我们进行了一个全面的比较，Mask R-CNN的现状与全面消融实验。

2. 我们使用COCO数据集[23]进行所有实验

3. 我们报告了标准的COCO指标，包括AP(超过IoU阈值的平均值)、AP50、AP75和APS、APM、APL(不同规模的AP)。

4. 除非另有说明，AP正在评估使用mask IoU。

5. 和之前的工作一样[3,22]，我们使用80k个train图像和35k个val图像子集(trainval35k)的组合进行训练，并报告剩余5k个val图像子集(minival)的烧蚀情况。

6. 我们还报告了test-dev[23]的结果，它没有公开的标签。

7. 出版后，我们会按建议将测试结果上载至test-std给公开排行榜

   #### Main Results

   1. 我们将Mask R-CNN与表1中最先进的实例分割方法进行比较。
   2. 我们的模型的所有实例化都优于以前最先进模型的基线变体。
   3. 这包括分别获得2015年COCO和2016年市场细分挑战赛冠军的MNC[7]和FCIS [21]
   4. 在没有任何附加功能的情况下，使用ResNet-101-FPN骨干的Mask R-CNN性能优于FCIS+++[21]，其中包括多尺度的训练/测试、水平翻转测试和在线硬示例挖掘(OHEM)[30]。
   5. 虽然超出了这项工作的范围，但我们期望许多这样的改进适用于我们的工作
   6. Mask R-CNN输出如图2和图4所示。
   7. Mask R-CNN即使在具有挑战性的条件下也能取得良好的效果。
   8. 在图5中，我们比较了Mask R-CNN基线和FCIS+++[21]。
   9. FCIS c++在重叠实例上展示了系统的工件，这表明它受到了实例分割的基本困难的挑战。蒙版R-CNN没有显示这样的人工制品。

   #### Ablation Experiments

   1. 我们运行了一些烧蚀分析掩模R-CNN
   2. 结果如表2所示，下面将详细讨论
   3. **Architecture:**表2a显示了带有各种骨干的Mask R-CNN
   4. 它得益于更深层次的网络(50 vs. 101)和包括FPN和ResNeXt3在内的高级设计
   5. 我们注意到，并非所有框架都能自动从更深层次或更高级的网络中获益(参见[17]中的基准测试)。
   6. **Multinomial vs. Independent Masks:**Mask R-CNN解耦mask和类预测:正如现有的box分支预测类标签一样，我们为每个类生成一个掩码，而不存在类之间的竞争(通过每个像素的sigmoid和二进制损失)。
   7. 在表2b中，我们将其与使用每个像素的softmax和多项损失进行比较(如FCN[24]中常用的那样)。
   8. 这种方法将蒙版任务和类预测任务耦合起来，导致蒙版AP严重下降(5.5分)。
   9. 这表明，一旦实例被分类为一个整体(通过box分支)，就足够预测一个二进制掩码而不考虑类别，这使得模型更容易训练。
   10. **Class-Specific vs. Class-Agnostic Masks:**我们的默认实例化预测类特定的掩码，比如每个类一个m×m掩模。
   11. 有趣的是，屏蔽R-CNN与类无关的面具(即。，预测一个单一的m×m输出(不考虑类)几乎是同样有效的:它有29.7个掩码AP，而在ResNet-50-C4上的特定类对应的掩码AP是30.3。
   12. 有趣的是，屏蔽R-CNN与类无关的面具(即。，预测一个单一的m×m输出(不考虑类)几乎是同样有效的:它有29.7个掩码AP，而在ResNet-50-C4上的特定类对应的掩码AP是30.3。
   13. 这进一步强调了我们在很大程度上解耦了分类和分割的方法中的劳动分工。
   14. **RoIAlign:**对我们提出的RoIAlign层的评估如表2c所示。
   15. 在这个实验中，我们使用了ResNet- 50-C4骨干，它具有stride 16。
   16. 与RoIPool相比，RoIAlign提高了AP约3点，大部分收益来自高IoU(AP75)。
   17. RoIAlign对最大/平均池不敏感;我们在论文的其余部分使用了平均值。
   18. 此外，我们比较了RoIWarp提出的MNC[7]，也采用双线性抽样
   19. 正如§3所讨论的，RoIWarp仍然量化RoI，与输入失去对齐。
   20. 从表2c中可以看出，RoIWarp的性能与RoIPool相当，但比RoIAlign差得多
   21. 这强调了正确的对齐是关键。
   22. 我们还评估了RoIAlign与ResNet-50-C5骨干，它有一个更大的步伐32像素
   23. 我们使用与图3(右)相同的头，因为res5头不适用。
   24. 表2d显示，RoIAlign极大地提高了掩模AP 7.3点，掩模AP75提高了10.5点(相对改进50%)。
   25. 此外，我们注意到使用RoIAlign时，使用stride-32 C5特性(30.9 AP)比使用stride-16 C4特性(30.3 AP，表2c)更准确。
   26. RoIAlign在很大程度上解决了长期以来使用大跨距特性进行检测和分割的难题。
   27. 最后，当与FPN一起使用时，RoIAlign显示了1.5 mask AP和0.5 box AP的增益，后者具有更好的多级步长。
   28. 对于需要更好对齐的关键点检测，即使使用FPN, RoIAlign也显示出较大的收益(表6)。
   29. **Mask Branch:**分割是一项像素对像素的任务，我们利用FCN来实现掩模的空间布局。
   30. 在表2e中，我们使用ResNet-50-FPN主干比较多层感知器(MLP)和FCNs
   31. 使用FCNs提供2.1掩码AP增益超过MLPs
   32. 我们注意到，我们选择这个主干，以便FCN头部的conv层没有经过预先训练，以便与MLP进行公平的比较。

   #### Bounding Box Detection Result

   1. 我们将Mask R-CNN与表3中最先进的COCO bounding-box对象检测进行比较。
   2. 对于这个结果，即使训练了完整的掩模R-CNN模型，在推断时也只使用分类和框输出(忽略掩模输出)。
   3. 使用ResNet-101-FPN的掩码R-CNN优于所有以前的最先进模型的基本变体，包括GRMI[17]的单模型变体，GRMI[17]是2016年COCO检测挑战的获胜者。
   4. 使用ResNeXt-101-FPN, Mask R-CNN进一步改进了结果，与[31](使用incep- resnet -v2- tdm)之前最好的单个模型条目相比，box AP的优势为3.0分。
   5. 为了进一步比较，我们训练了一个版本的Mask R-CNN，但是没有Mask分支，如表3所示，用“Faster R-CNN, RoIAlign”表示。
   6. 由于RoIAlign的存在，该模型的性能优于[22]中给出的模型
   7. 另一方面，box AP比Mask R-CNN低0.9个点。
   8. 因此，Mask R-CNN在盒检测方面的这种差距仅仅是由于多任务训练的好处。
   9. 最后，我们注意到Mask R-CNN在Mask和box AP之间有一个小的间隙，例如，在37.1 (Mask, Table 1)和39.8 (box, Table 3)之间有2.7个点。
   10. 这表明我们的方法在很大程度上缩小了对象检测和更具挑战性的实例分割任务之间的差距

   #### Timming

   1. **Inference:**我们训练了一个ResNet-101-FPN模型，该模型在RPN和Mask R-CNN阶段之间具有相同的特征，遵循Faster R-CNN[29]的4步训练。
   2. 该模型在Nvidia Tesla M40 GPU上以每张图像195ms的速度运行(加上将输出调整到原始分辨率所需的15ms CPU时间)，并在统计上实现了与未共享的掩模AP相同的掩模AP
   3. 我们还报告说，ResNet-101-C4变种需要400ms，因为它有一个更重的箱头(图3)，所以我们不建议在实践中使用C4变种
   4. 虽然Mask R-CNN速度很快，但是我们注意到我们的设计并没有针对速度进行优化，可以通过改变图像大小和提案号来实现更好的速度/精度权衡，这超出了本文的范围。
   5. **Training:**Mask R-CNN也是快速训练的
   6. 在COCO trainval35k上使用ResNet-50-FPN进行训练，在我们的同步8-GPU实现中需要32小时(每16个-图像小批处理0.72秒)，使用ResNet-101-FPN需要44小时。
   7. 事实上，当在train上训练时，快速原型可以在不到一天的时间内完成
   8. 我们希望这样的快速培训能够消除这一领域的一个主要障碍，并鼓励更多的人对这一具有挑战性的课题进行研究

### Mask R-CNN for Human Pose Estimation

1. 我们的框架可以很容易地扩展到人体姿态估计。
2. 我们将一个关键点的位置建模为一个热掩模，并采用掩模R-CNN来预测K个掩模，每种K个关键点类型(如左肩、右肘)对应一个掩模。
3. 该任务有助于演示掩模R-CNN的灵活性。
4. 我们注意到我们的系统利用了人体姿态的最小域知识，因为实验主要是为了证明掩模R-CNN框架的通用性
5. 我们希望领域知识(例如建模结构[4])能够补充我们的简单方法，但是这超出了本文的范围。
6. **Implementation Details:**我们对分割系统做了一些小的修改，当适应它的关键点。
7. 对于实例的每个K个关键点，训练目标都是一个单热m×m二元掩码，其中只有一个像素被标记为前景
8. 在训练过程中，对于每个可见的地面真值关键点，我们将在m2路softmax输出(鼓励检测单个点)上的交叉熵损失最小化。
9. 我们注意到，在实例分割中，K个关键点仍然是独立处理的。
10. 我们采用了ResNet-FPN变体，关键点头架构类似于图3(右)。
11. 关键点头由8个3×3 512-d conv层叠加而成，然后是deconv层和2×双线性上标，输出分辨率为56×56。
12. 我们发现一个相对较高的分辨率输出(与遮罩相比)是需要的关键点级定位精度。
13. 模型在所有包含注释关键点的COCO trainval35k图像上训练。
14. 为了减少过拟合，由于训练集较小，我们使用随机抽取的【64,800】像素的图像尺度来训练模型;推理是在800像素的单一尺度上进行的
15. 我们为90k迭代进行培训，从0.02的学习率开始，在60k和80k迭代时将学习率降低10。
16. 我们使用了阈值为0.5的边界盒非最大抑制。其他实现与§3.1相同。
17. **Experiments on Human Pose Estimation:**我们使用ResNet-50-FPN评估人员关键点AP (APkp)
18. 我们使用ResNet-101进行了实验，发现它也得到了类似的结果，这可能是因为更深层次的模型受益于更多的训练数据，但是这个数据集相对较小
19. 表4显示，我们的结果(62.7 APkp)比使用多级处理管道的COCO 2016关键点检测赢家[4]高0.9个点(见表4的说明)。
20. 我们的方法相当简单和快速
21. 更重要的是，我们有一个统一的模型，可以同时预测框、段和关键点，同时运行在5帧每秒。
22. 在test-dev上添加一个段分支(针对per-son类别)可以将APkp提高到63.1(表4)。
23. 多任务学习在微型计算机上的应用如表5所示。
24. 添加蒙版分支到box-only(即，更快的R-CNN)或只支持关键点的版本不断地改进这些任务。
25. 但是，添加keypoint分支会稍微减少box/mask AP，这表明虽然keypoint检测可以从多任务训练中获益，但它并不能帮助其他任务
26. 然而，学习所有这三个任务可以使一个统一的系统有效地同时预测所有的输出(图6)。
27. 我们还研究了RoIAlign对关键点检测的影响(表6)。
28. 虽然这个ResNet-50-FPN主干有更细的步长(例如，在最细的级别上有4个像素)，但是RoIAlign仍然比RoIPool有显著的改进，并将APkp提高了4.4个点
29. 这是因为关键点检测对定位精度更敏感。
30. 这再次表明对齐对于像素级定位是必不可少的，包括蒙版和关键点
31. 考虑到Mask R-CNN在提取对象边界框、掩码和关键点方面的有效性，我们希望它能够成为其他实例级任务的有效框架。

