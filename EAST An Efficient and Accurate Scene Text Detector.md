# EAST: An Efficient and Accurate Scene Text Detector

### Abstract

1. 以前的场景文本检测方法已经在各种基准测试中取得了很好的性能。
2. 然而，在处理具有挑战性的场景时，即使配备了深度神经网络模型，它们通常也不能满足要求，因为总体性能是由管道中多个阶段和组件的相互作用决定的
3. 在这项工作中，我们提出了一个简单而强大的管道，在自然场景中产生快速和准确的文本检测
4. 该管道通过单个神经网络直接预测全图像中任意方向和四边形的单词或文本行，消除了不必要的中间步骤(如候选聚合和单词划分)。
5. 我们的管道的简单性允许集中精力设计损失函数和神经网络架构。
6. 在ICDAR 2015、COCO-Text和MSRA-TD500等标准数据集上的实验表明，该算法在精度和效率上都明显优于目前最先进的方法。
7. 在ICDAR 2015数据集上，该算法在13.2fps、720p分辨率下，F-score达到0.7820

## Introduction

1. 近年来，提取和理解自然场景中所体现的文本信息变得越来越重要和流行，ICDAR系列比赛[30,16,15]的参赛人数之多，以及NIST[1]推出的TRAIT 2016测评，都证明了这一点。
2. 文本检测作为后续处理的前提，在文本信息提取和理解的整个过程中起着至关重要的作用。
3. 以往的文本检测方法[2,33,12,7,48]已经在该领域的各种基准上取得了良好的性能。
4. 文本检测的核心是区分文本和背景的特征设计。
5. 传统的特征提取方法是手工设计[5,25,40,10,26,45]来获取场景文本的属性，而基于深度学习的方法[3,13,11,12,7,48]直接从训练数据中学习有效的特征。
6. 然而，现有的方法，无论是传统的还是基于深度神经网络的，大多由几个阶段和组件组成，这可能是次优和耗时的。
7. 因此，这些方法的精度和效率还远远不能令人满意。
8. 在本文中，我们提出了一种快速准确的场景文本检测管道，它只有两个阶段
9. 该管道使用完全卷积网络(FCN)模型，该模型直接生成单词或文本行级别的预测，不包括冗余和缓慢的中间步骤
10. 生成的文本预测(可以是旋转矩形或四边形)被发送到非最大抑制以产生最终结果
11. 通过对标准基准的定性和定量实验，与现有方法相比，该算法的性能得到了显著提高，同时运行速度大大提高
12. 具体来说,该算法达到0.7820的f值在2015年ICDAR[15](0.8072测试时多尺度),0.7608 MSRA-TD500[40]和0.3945 COCO-Text[36],优于之前的最先进的算法性能,同时花更少的时间平均(13.2在720 p分辨率fps Titan-X GPU为我们表现最好的模型,为16.8 fps我们最快模型)。
13. 这项工作的贡献有三个方面:
14. 提出了一种场景文本检测方法，该方法由全卷积网络和NMS合并两个阶段组成。FCN直接生成文本区域，不包括冗余和耗时的中间步骤。
15. 管道可以灵活地生成字级或行级预测，其几何形状可以根据特定的应用程序旋转框或四边形。
16. 该算法在精度和速度上都明显优于现有的算法。

## Related Work

1. 场景文本检测与识别一直是计算机视觉领域的研究热点
2. 许多鼓舞人心的想法和有效的方法[5,25,26,24,27,37,11,12,7,41,42,31]已经被调查
3. 全面的综述和详细的分析可以在调查论文中找到[50,35,43]。
4. 本节将重点介绍与所提议的算法最相关的工作
5. 传统的方法依赖于手工设计的特性。
6. 基于笔画宽度变换(SWT)[5]和最大稳定极值区域(MSER)[25,26]的方法通常通过边缘检测或极值区域提取来寻找候选字符。
7. Zhang等人利用文本的局部对称性，设计了文本区域检测的各种特征。
8. .FASText[2]是一种快速文本检测系统，它采用并改进了著名的快速关键点检测器，用于笔画提取
9. 然而，这些方法在精度和适应性方面都落后于基于深度神经网络的方法，特别是在处理低分辨率和几何畸变等具有挑战性的场景时。
10. 近年来，场景文本检测领域进入了一个新的时代，基于深度神经网络的算法[11,13,48,7]逐渐成为主流。
11. Huang等人首先利用MSER发现候选项，然后利用深度卷积网络作为强分类器来剔除误报。
12. Jaderberg等人的方法[13]以滑动窗口的方式扫描图像，并使用卷积神经网络模型为每个尺度生成密集的热图。
13. 后来，Jaderberg等人[12]同时使用CNN和ACF来搜索候选单词，并使用回归进一步细化它们。
14. Tian等人开发了垂直锚，并构建了一个CNN-RNN联合模型来检测水平文本行
15. 与这些方法不同，Zhang等人提出利用FCN[23]生成热图，利用分量投影进行方向估计。
16. 这些方法在标准测试中取得了良好的性能
17. 然而，如图2(a-d)所示，它们大多由多个阶段和组件组成，如后过滤去除假阳性、候选聚合、行形成和分词等。
18. 大量的阶段和组件可能需要进行彻底的调优，导致性能不够理想，并增加了整个管道的处理时间。
19. 在本文中，我们设计了一种基于深fcn的管道，它直接针对文本检测的最终目标:单词或文本行级检测。
20. 如图2(e)所示，该模型摒弃了不必要的中间组件和步骤，允许端到端培训和优化
21. 所得到的系统，配备了一个单一的，轻加权神经网络，在性能和速度上都明显超过了以往的所有方法。

## Methodology

1. 该算法的关键部分是一个神经网络模型，该模型经过训练，可以直接从完整的图像中预测文本实例的存在及其几何形状。

2. 该模型是一个全卷积神经网络，适用于文本检测，输出密集的每像素预测的单词或文本行。

3. 这消除了中间步骤，如候选提案、文本区域形成和单词划分。

4. 后处理步骤仅包括对预测几何形状的阈值化和NMS

5. 该检测器被命名为EAST，因为它是一种高效、准确的场景文本检测管道

   ### Pipeline

   1. 图2(e)显示了我们的管道的高级概览。
   2. 该算法遵循DenseBox[9]的总体设计，将图像输入FCN，生成像素级文本分数图和几何图形的多个通道。
   3. 其中一个预测通道是一个像素值在[0,1]范围内的score map。
   4. 其余通道表示从每个像素的视图中包围单词的几何图形。
   5. 分数代表在相同位置预测的几何形状的置信度。
   6. 我们尝试了文本区域的两种几何形状，旋转框(RBOX)和四边形(QUAD)，并为每种几何设计了不同的损失函数
   7. 然后对每个预测区域应用阈值
   8. 然后将阈值应用于每个预测区域，其中得分超过预定义阈值的几何图形将被认为是有效的，并保存为以后的非最大抑制。
   9. NMS后的结果被认为是管道的最终输出。

   ### Network Design

   1. 在设计用于文本检测的神经网络时，必须考虑几个因素。

   2. 由于单词区域的大小，如图5所示，

   3. 如图5所示，由于单词区域的大小差异很大，因此判断大单词的存在需要神经网络后期的特征，而预测包围小单词区域的精确几何形状则需要早期的低水平信息

   4. 因此，网络必须使用来自不同级别的特性来满足这些需求。

   5. HyperNet[19]在特征映射上满足这些条件，但是在大型特征映射上合并大量通道会显著增加后期的计算开销。

   6. 为了解决这一问题，我们采用了u-shape[29]的思想，在保持上采样分支较小的同时，逐步合并特征图

   7. 我们最终得到的网络既能利用不同级别的特性，又能保持较小的计算成本

   8. 我们的模型示意图如图3所示。

   9. 该模型可分解为特征提取器主干、特征合并分支和输出层三个部分

   10. stem可以是在ImageNet[4]数据集上预先训练的卷积网络，具有交叉卷积和池化层。

   11. 从图像的stem中提取四层feature map，分别为输入图像的1/32、1/16、1/8和1/4。

   12. 图3所示为PVANet[17]。

   13. 在我们的实验中，我们也采用了著名的VGG16[32]模型，提取池2到池5后的特征映射。

   14. 在功能合并分支中，我们逐步合并它们
       $$
       g_i = \begin{cases} unpool(hi) \quad if \quad i \leq 3 \\ conv_{3x3} (h_i) \quad if \quad i=4\end{cases}
       $$

       $$
       h_i = \begin{cases} f_i \quad if \quad i=1 \\ conv_{3x3} (conv_{1x1}([g_{i-1};f_i])) \quad otherwise\end{cases}
       $$

       $g_i$是合并基，$h_i$是合并的feature map，运算符[·;·]表示沿通道轴线的级联

   15. 在每个合并阶段，从最后一个阶段开始的feature map首先被提供给一个unpooling层，使其大小加倍，然后与当前的feature map连接起来。

   16. 接下来，一个conv1×1瓶颈[8]减少了通道的数量，减少了计算量，接着是一个conv3×3，它融合了信息，最终产生这个合并阶段的输出

   17. 在最后一个合并阶段之后，conv3×3层生成合并分支的最终特征图，并将其提供给输出层

   18. 每个卷积的输出通道数如图3所示。

   19. 我们保持分支中卷积的通道数较小，这只增加了茎上一小部分计算开销，使网络计算效率更高

   20. 最后的输出层包含若干conv1×1操作，将32个feature map通道投射到score map Fs的一个通道和一个多通道几何体映射Fg中

   21. 几何输出可以是RBOX中的一个，也可以是QUAD中的一个，如表1所示

   22. 对于RBOX，几何由四个通道的轴向包围盒(AABB) R和一个通道旋转角度$\theta$表示。

   23. R的表达式与[9]中相同，其中4个通道分别表示从像素位置到矩形的上、右、下、左边界的4个距离。

   24. 对于QUAD Q，我们使用8个数字表示从四边形的四个角顶点{pi | i∈{1,2,3,4}}到像素位置的坐标移动

   25. 由于每个距离偏移量包含两个数字(xi，yi)，几何输出包含8个通道。

   ### Label Generation

   #### Score Map Generation for Quadrangle

   1. 在不失一般性的情况下，我们只考虑几何是四边形的情况

   2. 分数图上四边形的正面积设计为原始四边形的一个粗略缩小后的面积，如图4 (a)所示。

   3. 对于一个四边形Q = {|i∈{1,2,3,4}}，其中pi ={xi，yi}是四边形上按顺时针顺序排列的顶点

   4. 为了缩小Q，我们首先计算每个顶点pi的参考长度
      $$
      r_i = min(D(p_i, p_{(i-mod-4)+1}), D(p_i, p_{((i+3)mod4)+1}))
      $$
      D(p_i, p_j)是pi和pj的L2距离

   5. 我们首先缩小四边形的两条较长的边，然后再缩小两条较短的边

   6. 对于每一对相对的边，我们通过比较它们长度的平均值来确定“长”对。

   7. 对于每条边$<p_i,  p_{(i-mod-4)+1}>$，我们将它的两个端点沿边缘向内移动0.3ri和$0.3r_{(i-mod-4)+1}$，从而缩小它。

   #### Geometry Map Generation

   1. 正如在第3.2节中所讨论的，几何映射要么是RBOX要么是QUAD。
   2. RBOX的生成过程如图4 (c-e)所示。
   3. 对于那些文本区域以QUAD样式标注的数据集(例如，ICDAR 2015)，我们首先生成一个旋转矩形，它覆盖了面积最小的区域
   4. 然后对每个得分为正的像素，计算其到文本框4个边界的距离，并将其放入RBOX ground truth的4个通道中
   5. 对于QUAD ground truth, 8通道几何图中每个得分为正的像素点的值为其从四边形的4个顶点的坐标位移。

   ### Loss Functions

   1. 损失可表示为
      $$
      L = L_s + \lambda_gL_g
      $$

   2. 其中Ls和Lg分别表示score map和geometry的损失，$\lambda_g$表示两个损失之间的重要性。

   3. 在我们的实验中，我们将$\lambda_ g$ 设为1

   #### Loss for Score Map

   1. 在大多数最先进的检测管道中，训练图像都是经过精心的均衡采样和硬反挖掘处理，以处理目标对象分布的不均衡[9,28]。

   2. 这样做可能会提高网络性能。

   3. 然而，使用这种技术不可避免地会引入一个不可微级和更多的参数进行调优，以及更复杂的管道，这与我们的设计原则相矛盾。

   4. 为了简化训练过程，我们使用了[38]中引入的类平衡交叉熵
      $$
      L_s = balanced - xent(Y^-,Y^*) = - \beta Y^* log{Y^-}-(1-\beta)(1-Y^*)log(1-Y^-)
      $$

   5. $Y^- = F_s $是score map的预测，$Y^*$ 是ground truth

   6. 该参数为正、负样本间的平衡因子，由
      $$
      \beta = 1- \frac {\sum_{y^*∈Y^*}y^*} {|Y^*|}
      $$
      

   7. 这种均衡的交叉熵损失是姚等人[41]首次在文本检测中采用的，作为分数地图预测的目标函数。

   8. 我们发现它在实践中很有效

   #### Loss for Geometries

   1. 文本检测面临的一个挑战是，自然场景图像中的文本大小差异很大

   2. 直接使用L1或L2损失进行回归可以引导损失偏向于更大更长的文本区域。

   3. 由于我们需要为大文本区域和小文本区域生成准确的文本几何预测，因此回归损失应该是尺度不变的

   4. 因此，我们在RBOX回归的AABB部分采用IoU损失，在QUAD回归中采用尺度归一化smoothed d- l1损失

   5. **RBOX** 对于AABB部分，我们在[46]中采用IoU损失，因为它对不同尺度的对象是不变的
      $$
      L_{AABB} = -logIoU(R^-, R^*) = -log \frac {R^- ∩ R^*} {R^- ∪ R^*}
      $$
      $R^-$ 代表预测的AABB几何体， $R^*$ 是对应的ground truth

   6. 很容易看到,分割的矩形的宽度和高度$|R^- ∩ R^* |$ 是
      $$
      w_i = min(d_2^- , d_2^*) + min(d_4^-, d_4^*)
      $$

      $$
      h_i = min(d1^- , d_1^*) + min(d_3^-, d_3^*)
      $$

      d1、d2、d3、d4分别表示像素到对应矩形的上、右、下、左边界的距离

   7. 联合区域是
      $$
      |R^- ∪ R^*| = |R^-|+|R^*|-|R^- ∩ R^*|
      $$
      

   8. 因此，两者的交集/并集面积都可以很容易地计算出来。其次，旋转角损失计算为
      $$
      L_\theta(\theta^-, \theta^*) = 1-cos(\theta^- - \theta^*)
      $$
      $\theta^-$ 是预测的旋转角， $\theta^*$ 代表ground truth

   9. 最后，整体几何损失为AABB损失和角度损失的加权和，
      $$
      L_g = L_{AABB} = \lambda_\theta L_\theta
      $$
      $\lambda_\theta$ 在我们的实验中设置为10

   10. 注意，无论旋转角度如何计算LAABB

   11. 这可以看作是四边形IoU的近似，当角度被完美地预测时

   12. 虽然在训练,情况就不一样了,它仍然可以实施正确的梯度网络学习预测$R^-$ 。

   13. **QUAD** 我们在[6]中提出的平滑d- l1损失的基础上，增加了一个专门为字四边形设计的额外标准化项，它通常在一个方向上较长

   14. 设Q的所有坐标值为有序集
       $$
       C_Q = {x_1, y_1, x_2, y_2, ...,x_4, y_4}
       $$
       loss可以写作：
       $$
       L_g = L_{QUAD}(Q^-, Q^*) = min_{Q^- ∈P_{Q^*}} \sum_{c_i ∈C_Q , c_i^- ∈C_Q^-} \frac {smoothed_{L1}(c_i - c_i^-)} {8X N_Q^*}
       $$
       

   15. 归一化项$N_{Q^*}​$是四边形的短边长，由
       $$
       N_{Q^*} = min_{i=1}^4 D(p_i, p_{(i-mod-4)+1})
       $$
       

   16. PQ是具有不同顶点排序的Q的所有等价四边形的集合。

   17. 由于公共训练数据集中四边形的注释不一致，因此需要这种排序排列。

   ### Training

   1. 使用ADAM[18]优化器对网络进行端到端培训。
   2. 为了加快学习速度，我们从图像中统一采样512x512个作物，形成一个24个尺寸的小批。
   3. ADAM的学习率从1e-3开始，每27300个小批下降到十分之一，在1e-5停止。
   4. 网络被训练直到性能停止改进

   ### Locality-Aware NMS

   1. 为了形成最终的结果，阈值化后保留的几何图形应该由NMS合并
   2. 一个朴素的NMS算法运行在O(n2)中，其中n是候选几何图形的数量，这是不可接受的，因为我们面临着来自密集预测的成千上万个几何图形。
   3. 假设来自附近像素的几何图形高度相关，我们建议逐行合并几何图形，在合并同一行的几何图形时，我们将迭代地将当前遇到的几何图形与最后合并的几何图形合并
   4. 这种改进的技术在最好的情况下运行在O(n)中
   5. 即使最坏的情况与最简单的情况相同，只要局部性假设成立，算法在实践中运行得足够快。
   6. 算法1对该过程进行了总结
   7. 值得一提的是，在WEIGHTEDMERGE(g, p)中，合并后的四边形的坐标是由两个给定四边形的得分加权平均得到的
   8. 具体来说，如果a = WEIGHTEDMERGE(g, p)，则$a_i = V (g)g_i +V (p)p_i, V (a) = V (g)+V (p)$，其中ai为a下标的坐标之一，V (a)为几何a的得分。
   9. 事实上，有一个细微的区别，我们是在“平均”而不是像标准的NMS程序那样“选择”几何图形，作为一个投票机制，这反过来在提供视频时引入了稳定效果。
   10. 尽管如此，我们仍然使用“NMS”这个词来描述功能

   ### Experiments

   1. 为了与现有方法进行比较，我们对ICDAR2015、COCO-Text和MSRA-TD500三个公共基准进行了定性和定量实验。

   #### Benchmark Datasets

   1. **ICDAR 2015** 用于ICDAR 2015健壮阅读比赛[15]的挑战赛4。
   2. 共包含1500张图片，其中1000张用于培训，其余的用于测试
   3. 文本区域由四边形的四个顶点标注，与本文的四边形几何相对应。
   4. 我们还通过拟合一个具有最小面积的旋转矩形来生成RBOX输出
   5. 这些图像是由Google Glass以一种偶然的方式拍摄的
   6. 因此，文本在场景中可以是任意方向，或遭受运动模糊和低分辨率
   7. 我们还使用了ICDAR 2013年的229张训练图像
   8. .**COCO-Text**[36]是迄今为止最大的文本检测数据集。
   9. 它重用来自MS-COCO数据集[22]的图像。
   10. 总共注释了63,686张图像，其中43,686张被选择为训练集，其余20,000张用于测试。
   11. 词域以轴向包围框(AABB)的形式标注，这是RBOX的一种特殊情况。
   12. 对于这个数据集，我们将角度设置为零。
   13. 我们使用与ICDAR 2015相同的数据处理和测试方法。
   14. **MSRA-TD500**[40]是一个由300幅训练图像和200幅测试图像组成的数据集。
   15. 文本区域具有任意的方向，并在句级进行注释
   16. 与其他数据集不同，它包含中英文文本。
   17. 文本区域以RBOX格式注释
   18. 由于训练图像的数量太少，无法学习深度模型，我们还利用HUSTTR400数据集[39]中的400幅图像作为训练数据

   #### Base Networks

   1. 由于除COCO-Text外，所有的文本检测数据集相对于一般对象检测数据集来说都比较小[21,22]，因此如果对所有的基准采用单一网络，则可能会出现过拟合或欠拟合的情况。
   2. 我们在所有数据集上实验了三种不同的基本网络，使用不同的输出几何图形来评估所提议的框架
   3. 这些网络如表2所示。
   4. **VGG16**[32]被广泛用作许多任务的基础网络[28,38]，以支持后续特定于任务的finetuning，包括文本检测[34,48,49,7]。
   5. 这个网络有两个缺点:
      1. 这个网络的接受域很小。conv5 - 3输出中的每个像素只有196个接受域
      2. 这是一个相当大的网络
   6. **PVANET**是[17]中引入的一种轻量级网络，旨在替代fast - rcnn[28]框架中的特征提取器。
   7. 由于它对GPU来说太小，无法充分利用计算并行性，所以我们也采用了PVANET2x，使原来的PVANET的通道加倍，在比PVANET运行稍慢的情况下，利用了更多的计算并行性
   8. 这在第4.5节中有详细说明。
   9. 最后一个卷积层输出的接收域为809，比VGG16大得多。
   10. 模型在ImageNet数据集[21]上进行了预训练。

   ### Qualitative Results

   1. 图5给出了该算法的几个检测实例。
   2. 它能够处理各种具有挑战性的场景，如不均匀照明、低分辨率、变化的方向和透视失真
   3. 此外，由于NMS过程中的投票机制，该方法对具有多种文本实例的视频具有较高的稳定性。
   4. 该方法的中间结果如图6所示。
   5. 可以看出，训练后的模型生成了高度精确的几何图和分数图，其中很容易形成对不同方向文本实例的检测。

   ### Quantitative Result

   1. 如表3和表4所示，我们的方法在ICDAR 2015和COCO-Text上的性能大大超过了以前最先进的方法。
   2. 在ICDAR 2015 Challenge 4中，当图像按原始尺度进行馈送时，该方法的Fscore为0.7820。
   3. 在同一网络进行多尺度3测试时，我们的方法F-score达到0.8072，比最佳方法[41]的绝对值(0.8072 vs. 0.6477)高出近0.16。
   4. 通过比较VGG16网络的结果[34,48,41]，我们提出的方法在使用QUAD输出时的性能也比之前的工作[41]的性能好0.0924，在使用RBOX输出时的性能好0.116。
   5. 同时，这些网络非常有效，如第4.5节所示。
   6. 在COCO-Text中，该算法的三种设置都比之前的性能最好的[41]具有更高的精度。
   7. 具体来说，Fscore比[41]提高了0.0614,recall比[41]提高了0.053，这就证实了该算法的优点，因为COCO-Text是迄今为止最大、最具挑战性的基准测试。
   8. 注意，我们还将来自[36]的结果作为参考，但是这些结果实际上不是有效的基线，因为方法(A、B和C)用于数据注释。
   9. 通过对已有方法的改进，证明了一个简单的文本检测管道，可以直接针对最终目标，消除冗余过程，甚至可以打败那些集成了大型神经网络模型的复杂管道。
   10. 如表5所示，在MSRA-TD500上，我们方法的三种设置都取得了很好的效果
   11. 表现最好的(our +PVANET2x)的F-score略高于[41]。
   12. 与之前发表的最先进的系统Zhang等人的[48]方法相比，最佳性能(our +PVANET2x)的F-score提高了0.0208，精度提高了0.0428。
   13. 注意在MSRA-TD500算法配备VGG16执行更穷比PVANET和PVANET2x (0.7023 vs 0.7445和0.7608),主要原因是有效的接受域的VGG16小于PVANET PVANET2x,虽然MSRA-TD500需要的评估协议文本检测算法输出线级别而不是词的预测
   14. 此外，我们还在ICDAR 2013基准上评估了我们的+PVANET2x。
   15. 召回率为0.8267，准确率为0.9264,F-score为0.8737，与之前最先进的[34]方法相比，召回率为0.8298，准确率为0.9298,F-score为0.8769。

   ### Speed Comparison

   1. 总体速度比较如表6所示。
   2. 我们报告的数字是使用我们性能最好的网络运行ICDAR 2015数据集的500张原始分辨率(1280x720)测试图像的平均值。
   3. 这些实验是在一台服务器上进行的，使用的是带有Maxwell体系结构的NVIDIA Titan X图形卡和Intel E5-2670 v3 @ 2.30GHz CPU。
   4. 对于所提出的方法，后处理包括阈值化和NMS，其他方法应参考其原始论文。
   5. 虽然该方法的性能明显优于最先进的方法，但由于管道简单高效，计算成本非常低
   6. 从表6可以看出，我们方法中最快的设置运行速度为16.8 FPS，而最慢的设置运行速度为6.52 FPS。
   7. 即使是性能最好的模型our +PVANET2x运行速度为13.2 FPS。
   8. 这证实了我们的方法是最有效的文本检测器之一，在基准测试上实现了最先进的性能。

   ### Limitations

   1. 检测器能够处理的文本实例的最大大小与网络的接受域成正比。
   2. 这限制了网络预测更长的文本区域(如跨图像运行的文本行)的能力。
   3. 此外，由于垂直文本实例只占用ICDAR 2015训练集中的一小部分文本区域，该算法可能会错过或给出不精确的预测。

   ### Conclusion and Future Work

   1. 我们提出了一种场景文本检测器，它可以通过单一的神经网络直接从完整的图像中生成单词或行级预测。
   2. 通过加入适当的损失函数，检测器可以根据特定的应用，预测文本区域的旋转矩形或四边形
   3. 在标准基准上的实验证明，该算法在精度和效率上都明显优于以往的方法
   4. 未来研究的可能方向包括
   5. (1)调整几何公式，以便直接检测弯曲文本;
   6. (2)将检测器与文本识别器集成;
   7. (3)将该思想推广到一般对象检测

 