# FOTS: Fast Oriented Text Spotting with a Unified Network

## Abstract 

1. 偶然的场景文本定位被认为是文档分析社区中最困难和最有价值的挑战之一。
2. 大多数现有的方法将文本检测和识别作为独立的任务。
3. 在这项工作中，我们提出了一个统一的端到端可训练的快速文本定位(FOTS)网络，用于同时检测和识别，在两个互补的任务之间共享计算和视觉信息。
4. 具体地说，引入RoIRotate是为了在检测和识别之间共享卷积特征。
5. 得益于卷积共享策略，与基线文本检测网络相比，我们的FOTS的计算开销较小，联合训练方法使我们的方法比这两种阶段方法的性能更好。
6. 实验于2015年ICDAR ICDAR 2017 MLT,ICDAR 2013数据表明,该方法明显优于最先进的方法,进一步让我们开发第一个面向实时文字识别系统,超过以前所有最先进的结果超过5%在2015年ICDAR文字识别任务,同时保持22.6帧/秒。

## Introduction

1. 自然图像文本阅读由于其在文档分析、场景理解、机器人导航和图像检索等方面的大量实际应用，越来越受到计算机视觉界的关注[49、43、53、44、14、15、34]。
2. 虽然前人的工作在文本检测和文本识别方面都取得了很大的进展，但由于文本模式的多样性和背景的高度复杂性，文本识别仍然具有挑战性
3. 场景文本阅读中最常见的方法是将其分为文本检测和文本识别两部分，分别处理为两个独立的任务[20,34]。
4. 基于深度学习的方法在这两方面都占主导地位
5. 在文本检测中，通常使用卷积神经网络从场景图像中提取特征图，然后使用不同的解码器对区域进行解码[49,43,53]。
6. 而在文本识别中，序列预测网络是在文本区域上逐区域进行的[44,14]。
7. 这将导致大量的时间开销，特别是对于具有多个文本区域的图像。
8. 另一个问题是，它忽略了检测和识别过程中共享的视觉线索之间的相关性。
9. 单个检测网络不能由文本识别中的标签进行监控，反之亦然
10. 在本文中，我们建议同时考虑文本检测和识别。
11. 实现了面向文本的快速定位系统(FOTS)。
12. 与以往的两阶段文本识别方法相比，我们的方法通过卷积神经网络学习到更好的特征，卷积神经网络在文本检测和文本识别中是共享的。
13. 由于特征提取通常花费大部分时间，因此将计算量缩减到单个检测网络，如图1所示。
14. 检测与识别的关键是ROIRotate，它根据定向检测的边界框从特征图中获取合适的特征。
15. 结构如图2所示。
16. 首先利用共享卷积提取特征图
17. 在特征图的基础上，建立了基于全卷积网络的面向文本检测分支，对检测边界框进行预测。
18. RoIRotate操作符从特征图中提取与检测结果对应的文本建议特征。
19. 然后将文本建议特征输入循环神经网络(RNN)编码器和连接时间分类(CTC)解码器[9]进行文本识别。
20. 由于网络中的所有模块都是可微的，因此可以对整个系统进行端到端的训练。
21. 据我们所知，这是第一个面向文本检测和识别的端到端可训练框架。
22. 我们发现，该网络无需复杂的后处理和超参数调整，就可以很容易地进行训练
23. 其贡献摘要如下：
    1. 我们提出了一种端到端可训练的文本快速定位框架。
    2. 通过共享卷积特征，网络可以同时检测和识别文本，且计算量小，提高了实时性。
    3. 引入了一种新的可微算子RoIRotate，用于从卷积特征图中提取面向文本区域
    4. 该操作将文本检测和识别统一到一个端到端管道中。
    5. FOTS在许多文本检测和文本定位基准上显著超过了最先进的方法，包括ICDAR 2015[26]、ICDAR 2017 MLT[1]和ICDAR 2013[27]。

## Related Work

1. 文本识别是计算机视觉和文档分析中的一个活跃课题。
2. 在这一节中，我们简要介绍了相关的工作，包括文本检测，文本识别和文本识别方法的结合。

### Text Detection

1. 大多数传统的文本检测方法都将文本看作是字符的组合
2. 这些基于字符的方法首先本地化图像中的字符，然后将它们分组为单词或文本行。
3. 基于滑动窗口的方法[22,28,3,54]和基于连接组件的方法[18,40,2]是传统方法中具有代表性的两大类。
4. 近年来，人们提出了许多基于深度学习的图像词检测方法。
5. 田等人采用垂直锚定机制预测定宽序列方案，并将其连接起来(ctpn)
6. Ma等人通过提出旋转RPN和旋转RoI池，为任意方向的文本引入了一种基于旋转的框架(rrpn)
7. Shi等人首先预测文本片段，然后使用链接预测将它们链接到完整的实例中。
8. Zhou等人[53]采用密集预测和一步后处理的方法，He等人[15]提出了面向多场景文本检测的深度直接回归方法。

### Text Recognition

1. 一般情况下，场景文本识别的目的是从规则裁剪但长度可变的文本图像中解码标签序列。
2. 以前的大多数方法[8,30]捕获单个字符，然后对错误分类的字符进行细化
3. 除了字符级方法外，近年来的文本区域识别方法可分为三类:基于单词分类的方法、基于序列到标签解码的方法和基于序列到序列模型的方法。
4. Jaderberg等人[19]将单词识别问题提出为一个传统的多类分类任务，具有大量的类标签。
5. Su等人将[48]帧文本识别作为一个序列标记问题，其中RNN建立在HOG特征的基础上，采用CTC作为解码器
6. Shi等人[44]和He等人[14]提出了深度递归模型对最大输出CNN特征进行编码，并采用CTC对编码序列进行解码
7. Fujii等人提出了一种编码和摘要网络来执行行级脚本识别。
8. Lee等人使用基于注意力的sequenceto- sequence结构自动聚焦于所提取的CNN特征，并隐式学习RNN中包含的字符级语言模型。
9. 为了处理不规则输入图像，Shi等人的[45]和Liu等人的[37]引入了空间注意机制，将扭曲的文本区域转换为适合识别的正则位姿

### Text Spotting

1. 大多数以前的文本识别方法首先使用文本检测模型生成文本建议，然后使用单独的文本识别模型识别它们。
2. Jaderberg等人首先使用集成模型生成具有高召回率的整体文本建议，然后使用单词分类器进行单词识别。
3. Gupta等人[10]训练了一个用于文本检测的全卷积回归网络，并采用[19]中的单词分类器进行文本识别
4. 廖等人使用基于SSD[36]的文本检测方法和基于CRNN[44]的文本识别方法。
5. 最近Li等人[33]提出了一种端到端文本识别方法，该方法利用受RPN[41]启发的文本建议网络进行文本检测，并利用LSTM的注意机制进行文本识别[38,45,3]
6. 与它们相比，我们的方法主要有两个优势:
   1. 我们引入RoIRotate并使用完全不同的文本检测算法来解决更复杂和困难的情况，而他们的方法只适用于水平文本
   2. 我们的方法在速度和性能上都比他们的好得多，特别是几乎免费的文本识别步骤，使得我们的文本识别系统能够以实时的速度运行，而他们的方法处理600×800像素的输入图像需要大约900ms的时间。

## Methodology

1. FOTS是一个端到端可训练的框架，可以同时检测和识别自然场景图像中的所有单词
2. 它由共享卷积、文本检测分支、RoIRotate操作和文本识别分支四部分组成。

### Overall Architecture

1. 我们的框架概述如图2所示。
2. 文本检测分支和识别分支共享卷积特征，共享网络结构如图3所示。
3. 共享网络的主干是ResNet-50[12]。
4. 受特征金字塔网络[35]的启发，我们将低层特征图和高层语义特征图连接起来。
5. 共享卷积生成的特征图的分辨率是输入图像的1/4。
6. 文本检测分支使用共享卷积产生的特征输出文本的密集逐像素预测。
7. 提出的RoIRotate利用检测分支生成的面向文本区域的建议，在保持原始区域长径比的同时，将相应的共享特征转换为固定高度表示
8. 最后，文本识别分支识别区域提案中的单词。
9. 采用CNN和LSTM对文本序列信息进行编码，然后采用CTC解码器
10. 我们的文本识别分支的结构如表1所示。

### Text Detection Branch

1. 受[53,15]的启发，我们采用全卷积网络作为文本检测器。

2. 由于自然场景图像中有大量的小文本框，我们在共享卷积中将feature map的大小从原始输入图像的1/32提升到1/4。

3. 在提取了共享特征后，一个卷积被应用于输出密集的每像素预测的文本。

4. 第一个通道计算每个像素为正样本的概率。

5. 与[53]类似，原始文本区域缩小后的像素被认为是正的。

6. 对于每个正样本，以下4个通道预测其到包含该像素的边界框的顶、底、左、右的距离，最后一个通道预测相关边界框的方向

7. 对这些阳性样本进行阈值化和NMS处理，得到最终的检测结果。

8. 在我们的实验中，我们观察到许多类似于文本笔划的模式是很难分类的，例如栅栏、格子等。

9. 为了更好地区分这些模式，我们采用了在线硬例子挖掘(OHEM)[46]，这也解决了这个问题

   阶级不平衡问题

10. 这为ICDAR 2015数据集提供了约2%的F-measure改进

11. 检测分支loss函数由文本分类项和边界框回归项两部分组成。

12. 文本分类项可以看作是向下采样的分数图的像素级分类损失

13. 只有原始文本区域的缩小版本被认为是正的区域，而边界框与缩小版本之间的区域被认为是“不关心”的区域，不会造成分类的损失。

14. 将得分图中OHEM选取的阳性元素集合表示为$\Omega$，分类损失函数可表示为：
    $$
    L_{cls}=\frac {1} {|\Omega|} \sum _{x∈\Omega}H(p_x, p_x^*) = \frac {1} {|\Omega|} \sum _{x∈\Omega}(-p_x^*logp_x - (1-p_x^*)log(1-p_x))
    $$
    |·|是集合中元素的数量， $H(p_x, p_x^*)$代表预测的socore map $p_x$和表示文本和非文本的的$p_x^*$（只有0，1二值）间的cross entropy loss，

15. 对于回归损失，我们采用[52]中的IoU损失和[53]中的转角损失，因为它们对物体形状、尺度和方向的变化具有鲁棒性：
    $$
    L_{reg} =\frac {1} {|\Omega|} \sum _{x∈\Omega}IoU(R_x, R_x^*)+\lambda _\theta (1-cos(\theta _x, \theta_x^*))
    $$
    $IoU(R_x, R_x^*)$是预测出来的bounding box$R_x$和ground truth$R_X^*$之间IoU loss

16. 第二项是旋转角度损失， $\theta_x$和$\theta_x^*$分别代表预测方向和ground truth的方向

17. 超参数$\lambda_\theta$在实验中设为10

18. 因此，完整的检测损耗可以写成:
    $$
    L_{detect} = L_{cls}+\lambda_{reg}L_{reg}
    $$
    超参数$\lambda_{reg}$平衡两个loss，在我们的实验中设为1

### RoIRotate

1. RoIRotate对定向的特征区域进行变换，得到轴向对齐的特征图，如图4所示。

2. 在这项工作中，我们固定了输出高度，并保持长宽比不变，以处理文本长度的变化

3. 与RoI池[6]和RoIAlign[11]相比，RoIRotate为提取感兴趣区域的特性提供了更通用的操作

4. 并与RRPN[39]中提出的RRoI池进行了比较。

5. RRoI池通过最大池将旋转区域转换为固定大小的区域，同时使用双线性插值计算输出值。

6. 该操作避免了RoI与提取的特征之间的不对齐，并且使输出特征的长度可变，更适合于文本识别。

7. 这个过程可以分为两个步骤

8. 首先，通过文本提案的预测或地面真值坐标计算仿射变换参数。

9. 然后分别对每个区域的共享特征映射进行仿射变换，得到文本区域的正则水平特征映射

10. 第一步可以表示为:

    ![1555400397556](D:\PaperReading\assets\1555400397556.png)

    M是仿射变换矩阵。$h_t, w_t$表示仿射变换后feature maps的高度（我们的设置中等于8）和宽度。（x, y）代表这共享feature maps上的点的坐标，(t,b,l,r)分别代表到top, bottom, left, right的距离，$\theta$ 代表角度。(t,b,l,r)和$\theta$由ground truth或者检测分支给出。

11. 利用变换参数，利用仿射变换很容易得到最终的RoI特征

    ![1555401008812](D:\PaperReading\assets\1555401008812.png)

    ![1555401053084](D:\PaperReading\assets\1555401053084.png)

    ![1555401071517](D:\PaperReading\assets\1555401071517.png)

    $V_{ij}^c$是在channel c的location (i,j)的输出值， $U_{nm}^c$是channel c的location(n,m)的输入值。$h_s, w_s$ 代表这输入的高和宽，$\Phi_x , \Phi_y$是通用抽样kernel k()的参数，定义了插值方法，特别是双线性插值在这项工作

12. 由于文本建议的宽度可能不同，在实际应用中，我们将特征映射填充到最长的宽度，忽略了识别loss函数中的填充部分。

13. Spatial transformer network[21]采用了类似的仿射变换，但通过不同的方法获取变换参数，主要用于图像域，即图像本身的变换

14. RoIRotate以共享卷积生成的feature map作为输入，生成所有文本提案的feature map，具有固定的高度和不变的纵横比。

15. 与目标分类不同，文本识别对检测噪声非常敏感

16. 由于预测文本区域的一个小错误就可能导致多个字符被截断，不利于网络训练，因此在训练中我们使用了地面真实文本区域而不是预测文本区域

17. 测试时，采用阈值和NMS对预测文本区域进行过滤。

18. 在RoIRotate之后，转换后的特征映射被提供给文本识别分支。

### Text Recognition Branch

1. 文本识别分支的目的是利用共享卷积提取的区域特征和RoIRotate转换的区域特征来预测文本标签
2. 考虑到文本区域中标签序列的长度，通过对原始图像进行共享卷积，LSTM的输入特性沿宽度轴仅减少两次(如第3.2节所述减少到1/4)。
3. 否则，在紧凑的文本区域，特别是窄形字符区域，可识别的特征将被消除。
4. 我们的文本识别分支由VGGlike[47]序列卷积、仅沿高度轴缩减的池、一个双向LSTM[42, 16]、一个全连接和最终的CTC解码器[9]组成。
5. 首先，将空间特征沿高度轴依次进行卷积和聚类，并进行降维处理，提取更高层次的特征
6. 为了简单起见，这里所有报告的结果都基于类似于vgg的顺序层，如表1所示。
7. 接下来，提取更高层次feature map $L ∈R^{C*H*W}$被排列成时间形式，以$l_1,......,l_w ∈R^{C*H}$的形式,输入RNN进行encoding
8. 这里，我们使用双向LSTM（每个方向有输出通道D=256），来捕获输入序列特性的范围依赖性
9. 然后，隐藏状态$h_1,...,h_w ∈R^D$计算两个方向上的时间步长的求和，并且送入全连接层中，得到每个状态在字符类S上的分布$x_t ∈ R^{|S|}$
10. 为了避免像ICDAR 2015这样的小训练数据集过度拟合，我们在完全连接之前添加了**dropout**。
11. 最后，利用CTC将框架分类分数转换为标签序列。
12. 

