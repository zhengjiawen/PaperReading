# Detecting Text in Natural Image with Connectionist Text Proposal Network

### Abstract

1. 我们提出了一种新的连接主义文本建议网络(CTPN)，它可以精确地定位自然图像中的文本行
2. CTPN直接在卷积特征图中检测一系列精细文本建议中的文本行。
3. 我们开发了一种垂直锚定机制，可以联合预测每个固定宽度提案的位置和文本/非文本得分，大大提高了定位精度
4. 序列方案自然由递归神经网络连接，该神经网络与卷积网络无缝结合，形成端到端的可训练模型
5. 这使得CTPN能够探索丰富的图像上下文信息，使得它能够强大地检测极其模糊的文本。
6. CTPN在多尺度、多语言文本上工作可靠，无需进一步的后处理，有别于以往自下而上的多步后过滤方法
7. 在ICDAR 2013和2015的基准上，它分别达到了0.88和0.61 F-measure，远远超过了最近的结果[8,35]
8. 通过使用非常深的VGG16模型，CTPN的计算效率为0.14 s/image

## Introduction

1. 近年来，阅读自然图像文本在计算机视觉领域受到越来越多的关注[1,8 - 11,14,15,28,32,35]。

2. 这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。

3. 它包括两个子任务:文本检测和识别

4. 这项工作的重点是检测任务[1,14,28,32]，这比在裁剪良好的单词图像上进行识别任务更具挑战性

5. 文本模式的多样性和背景的高度杂乱是准确定位文本的主要挑战。

6. 目前的文本检测方法大多采用自底向上的管道

7. 它们通常从低级字符或笔画检测开始，然后通常是一系列后续步骤:非文本组件过滤、文本行构建和文本行验证

8. 这些多步骤自底向上的方法通常比较复杂，鲁棒性和可靠性较差

9. 它们的性能在很大程度上依赖于字符检测的结果，并提出了连接连通域或滑动窗口方法

10. 这些方法通常探索底层特性(例如，基于SWT[3,13]、MSER[14,23,33]或HoG[28])来区分文本候选对象和背景

11. 然而，在没有上下文信息的情况下，单独识别单个笔画或字符并不可靠

12. 例如，人们识别字符序列比识别单个字符序列更有信心，特别是当一个字符非常模糊时。

13. 这些限制常常导致字符检测中出现大量的非文本组件，导致在以下步骤中处理这些组件时遇到主要困难。

14. 此外，正如[28]所指出的，在自底向上的管道中，这些错误检测很容易顺序累积。

15. 为了解决这些问题，我们利用强大的深层特性来直接检测卷积图中的文本信息

16. 我们开发了文本锚定机制，可以精确地预测文本在精细尺度上的位置。

17. 然后，提出了一种网络内递归结构，将这些精细的文本提案按顺序连接起来，使它们能够编码丰富的上下文信息。

18. 深度卷积神经网络(CNN)近年来已大大提高了一般对象检测的水平[5,6,25]。

19. 目前最先进的方法是使用 faster Region- cnn (R-CNN)系统[25]，其中提出了一个Region Proposal Network (RPN)，可以直接从卷积feature map中生成高质量的类无关对象提案

20. 然后将RPN的建议输入到Fast-R-CNN[5]模型中进行进一步的分类和细化，从而获得最先进的通用对象检测性能

21. **然而，这些通用的目标检测系统很难直接应用于场景文本检测，这通常要求较高的定位精度。**

22. 在泛型对象检测中，每个对象都有一个定义良好的封闭边界[2]，而这种定义良好的边界可能不存在于文本中，因为文本行或单词由许多单独的字符或笔画组成。

23. 对于对象检测来说，一个典型的正确检测定义得比较松散，例如，被检测的边界框与其ground truth之间的>0.5重叠(例如PASCAL标准[4])，因为人们可以很容易地从它的主要部分识别出一个对象

24. 相比之下，全面阅读文本是一项细粒度的识别任务，需要对文本行或单词的整个区域进行正确的检测

25. 因此，文本检测通常需要更准确的定位，从而产生了不同的评价标准，如wolf的标准[30]，它通常被文本基准使用[19,21]。

26. 在这项工作中，我们通过将RPN体系结构[25]扩展到精确的文本行本地化来填补这一空白。

27. 我们提出了几个技术发展，使通用对象检测模型适合我们的问题

28. 为了更进一步，我们提出了一种网络内循环机制，该机制允许我们的模型直接检测卷积图中的文本序列，避免了使用额外的昂贵CNN检测模型的进一步后处理

    #### Contributions

    1. 我们提出了一种新的Connectionist Text Proposal Network(CTPN)，它可以直接定位卷积层中的文本序列。
    2. 这克服了以前基于字符检测的自底向上方法所带来的一些主要限制。
    3. 利用强深卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。
    4. 它的主要贡献如下:
    5. 首先，我们将文本检测问题转换为对一系列精细文本建议的本地化。
    6. 我们开发了一个锚点回归机制，可以联合预测垂直位置和每个文本提案的文本/非文本得分，从而获得非常好的定位精度
    7. 这偏离了对整个目标的RPN预测，难以提供满意的定位精度。
    8. 其次，我们提出一种网络内递归机制，将卷积特征图中的顺序文本建议优雅地连接起来
    9. 这种连接使我们的检测器能够探索有意义的文本行上下文信息，使它能够可靠地检测极具挑战性的文本
    10. 第三，这两种方法无缝集成，以满足文本序列的性质，从而形成统一的端到端可训练模型
    11. 我们的方法能够在一个过程中处理多尺度、多语言的文本，避免了进一步的后过滤或细化。
    12. 第四，我们的方法在多个基准上取得了新的最先进的结果，显著改善了最近的结果(例如，2013年ICDAR上[8]的0.88 F-measure超过0.83,ICDAR2015上[35]的0.61 F-measure超过0.54)
    13. 此外，它的计算效率很高，通过使用非常深的VGG16模型，可以获得0.14 s/图像运行时间(在ICDAR 2013上)

    ## Related Work

    1. **Text detection**.以往的场景文本检测工作主要是基于笔画或字符检测的自底向上方法。
    2. 它们可以大致分为两类，基于connected-components(CCs,连通域)的方法和基于sliding-window（滑动窗口）的方法
    3. 基于CCs的方法通过快速过滤器对文本和非文本像素进行区分，然后利用强度、颜色、梯度等底层属性贪婪地将文本像素分组为笔画或候选字符[3,13,14,32,33]。
    4. 基于滑动窗口的方法通过在图像中密集移动多尺度窗口来检测候选字符
    5. 字符或非字符窗口由预先训练的分类器通过使用手工设计的特性[28,29]或最近的CNN特性[16]进行识别。
    6. 然而，这两种方法的字符检测性能普遍较差，在后续的组件滤波和文本行构造步骤中积累了错误。
    7. 此外，鲁棒过滤非字符组件或自信地验证检测到的文本行本身甚至是困难的[1,14,33]。
    8. 另一个限制是，通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上非常昂贵。
    9. **Object detection**.卷积神经网络(CNN)最近大幅提升了通用对象检测技术[5,6,25]
    10. 一种常用的策略是使用廉价的底层特征生成大量的对象建议，然后使用一个强大的CNN分类器对生成的建议进行进一步的分类和细化
    11. 选择性搜索(SS)[4]是目前主流的对象检测系统(如区域CNN (R-CNN)[6]及其扩展[5])中应用最广泛的一种方法，它生成与类无关的对象建议。
    12. 最近，Ren等人提出了一种更快的R-CNN目标检测系统
    13. 他们提出了一种区域建议网络(RPN)，可以直接从卷积特征图生成高质量的类无关对象建议
    14. 通过共享卷积计算，RPN速度很快。
    15. 然而，RPN的建议并不具有区别性，需要通过一个额外的昂贵的CNN模型，如快速的R-CNN模型[5]，进一步细化和分类。
    16. 更重要的是，文本与一般对象有很大的不同，这使得直接将一般对象检测系统应用于这种高度领域特定的任务非常困难

    ## Connectionist Text Proposal Network

    1. 本节介绍Connectionist Text Proposal Network(CTPN)的详细信息。

    2. 它包括三个关键的贡献，使它可靠和准确的文本本地化:检测文本在精细规模的建议，经常性的连接主义文本建议，和侧精炼

       ### Detection Text in Fine-Scale Proposal

       1. 与区域建议网络(RPN)[25]类似，CTPN本质上是一个完全卷积的网络，允许任意大小的输入图像

       2. 它通过在卷积特征图中密集滑动一个小窗口来检测文本行，并输出一系列精细尺度(如固定16像素宽度)的文本建议，如图1(b)所示。

       3. 我们以非常深的16层vggNet (VGG16)[27]为例来描述我们的方法，这种方法很容易适用于其他深度模型

       4. CTPN的结构如图1(a)所示。

       5. 我们使用一个小的空间窗口3×3来滑动最后一个卷积层的feature map(例如VGG16的conv5)。

       6. conv5 feature map的大小由输入图像的大小决定，总步幅和接受域分别固定为16和228像素。

       7. 整个步幅和接受域都是由网络结构决定的。

       8. 在卷积层中使用滑动窗口可以实现卷积计算的共享，这是降低基于滑动窗口方法计算量的关键。

       9. 通常，滑动窗口方法采用多尺度窗口来检测不同大小的对象，其中一个窗口尺度固定在相同大小的对象上。

       10. 在[25]中，Ren等人提出了一种有效的锚回归机制，允许RPN用一个单尺度窗口检测多尺度对象。

       11. 关键的洞见是，通过使用许多灵活的锚，单个窗口能够预测大范围尺度和宽高比的对象

       12. 我们希望将这种有效的锚机制扩展到文本任务

       13. 然而，文本与泛型对象有本质上的不同，**泛型对象通常具有定义良好的封闭边界和中心**，允许从整个对象甚至是它的一部分来推断整个对象。

       14. **文本是一个没有明显封闭边界的序列。**

       15. 它可能包含多个层次的组件，如笔画、字符、单词、文本行和文本区域，这些组件之间没有明显的区别

       16. 文本检测是在单词或文本行级别定义的，因此将其定义为单个对象(例如检测单词的一部分)很容易产生错误的检测。

       17. 因此，直接预测文本行或单词的位置可能是困难的或不可靠的，很难获得满意的准确性

       18. 图2显示了一个例子，其中RPN直接训练用于定位图像中的文本行。

       19. 我们寻找文本的一个独特属性，它能够很好地概括到所有级别的文本组件

       20. 我们观察到，RPN的单词检测很难准确预测单词的水平方向，因为单词内的每个字符都是孤立或分离的，很难找到单词的起始和结束位置。

       21. 显然，文本行是一个序列，这是文本和泛型对象之间的主要区别

       22. 将文本行看作是细尺度文本建议序列是很自然的，其中每个建议通常表示文本行的一小部分，例如16像素宽度的文本块。

       23. 每个提案可以包括一个或多个笔画、一个字符的一部分、一个或多个字符，等等。

       24. 我们认为，仅仅预测每一项建议的垂直位置将更为准确，通过固定其可能更难预测的水平位置。

       25. 与RPN相比，这减少了搜索空间，RPN预测一个对象的4个坐标

       26. 我们开发了一种垂直锚定机制，可以同时预测文本/非文本评分和每个细尺度提案的y轴位置。

       27. 检测一般固定宽度的文本建议也比识别单个字符更可靠，后者很容易与字符的一部分或多个字符混淆

       28. 此外，在一系列固定宽度的文本建议中检测文本行也可以可靠地工作在多个尺度和多个纵横比的文本上。

       29. 此外，在一系列固定宽度的文本建议中检测文本行也可以可靠地工作在多个尺度和多个纵横比的文本上。

       30. 为此，我们设计了以下精细文本提案

       31. 我们的探测器密集地调查conv5中的每个空间位置

       32. 文本建议被定义为固定宽度为16像素(在输入图像中)。

       33. 这相当于让探测器密集地通过conv5映射，其中总步幅正好为16个像素。

       34. 然后我们设计了k个垂直锚来预测每个方案的y坐标。

       35. k个锚点的水平位置相同，宽度固定为16像素，但垂直位置的k个高度不同。

       36. 在我们的实验中，我们为每个proposal使用10个锚点，k = 10，在输入图像中，每个锚点的高度从11到273像素不等(每次÷0.7)

       37. 在我们的实验中，我们为每个proposal使用10个锚点，k = 10，在输入图像中，每个锚点的高度从11到273像素不等(每次÷0.7)

       38. 显式垂直坐标是由一个建议边界框的高度和y轴中心来测量的

       39. 我们计算相对预测垂直坐标(v)相对于锚的包围盒位置，
           $$
           v_c = (c_y - c_y^a)/h^a, \quad v_h = log(h/h^a)
           $$

           $$
           v_c^* = (c_y^*-c_y^a)/h^a, \quad v_h^* = log(h^*/h^a)
           $$

           $V = \{v_c, v_h\}$和$V^* = \{v_c^*, v_h^*\}$分别是相对预测坐标和ground truth坐标

       40. $c_y^a$和$h^a$是锚框的中心(y轴)和高度，可以通过输入图像预先计算

       41. $c_y$和h为输入图像中预测的y轴坐标，$c_y^*$和$h^*$为地面真实坐标。

       42. 因此，每个预测文本提案都有一个大小为h×16的边界框(在输入图像中)，如图1(b)和图2(右)所示。

       43. 一般来说，一个文本提案远小于它的有效接受范围228x228

       44. 检测过程总结如下

       45. 给定一个输入图像，我们有W×H×C conv5 feature maps(使用VGG16模型)，其中C是feature maps或channel的数量，W×H是空间排列

       46. 当我们的检测器将一个3×3的窗口密集地通过conv5时，每个滑动窗口的卷积特征为3×3×C，用于产生预测。

       47. 对于每个预测，水平位置(x坐标)和k锚点位置都是固定的，可以通过将conv5中的空间窗口位置映射到输入图像上来预先计算。

       48. 我们的检测器输出每个窗口位置k个锚点的文本/非文本分数和预测的y坐标(v)。

       49. 检测到的文本建议由文本/非文本得分为>0.7的锚点生成(带有非最大抑制)。

       50. 通过设计的垂直锚点和精细尺度检测策略，我们的检测器能够使用单尺度图像处理大范围尺度和宽高比的文本行。

       51. 这进一步减少了计算量，同时预测了文本行的精确本地化。与RPN或更快的RCNN系统[25]相比，我们的精细尺度检测提供了更详细的监督信息，自然会导致更准确的检测。