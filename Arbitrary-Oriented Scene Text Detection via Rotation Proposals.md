# Arbitrary-Oriented Scene Text Detection via Rotation Proposals

### Abstract

1. 提出了一种新的基于旋转(rotation-based)的自然场景图像中任意方向文本检测框架。
2. 我们提出了一种基于文本倾斜角信息的旋转区域提案网络。
3. 然后利用角度信息进行包围盒回归，使提案在方向上更准确地融入文本区域
4. 针对文本区域分类器的特点，提出了旋转兴趣区域池层，将任意方向的建议投影到特征图中
5. 整个框架建立在基于区域提议的体系结构之上，与以往的文本检测系统相比，保证了面向任意文本检测的计算效率。
6. 我们在三个真实场景文本检测数据集上使用基于旋转的框架进行了实验，证明了该框架在有效性和效率方面优于之前的方法。

## Introduction

1. 文本检测的目的是识别给定图像的文本区域，是实现视觉分类[1]、[2]、视频分析[3]、[4]和移动应用[5]等多媒体任务的重要前提。
2. 尽管目前已有一些商用光学字符识别(OCR)系统用于记录文本或互联网内容，但由于光线不均匀、模糊、透视失真、方向等复杂情况，对自然场景图像中的文本检测具有挑战性。
3. 近年来，文本检测任务(如[6]-[16])得到了广泛的关注。
4. 尽管这些方法已经显示出了很好的结果，但是它们中的大多数都依赖于水平或接近水平的注释并返回水平区域的检测。
5. 然而，在实际应用中，大量的文本区域不是水平的，即使应用非水平对齐的文本行作为轴对齐的建议也可能不准确。
6. 因此，水平定向方法在实际应用中不能得到广泛的应用。
7. 近年来，针对任意文本检测[17]-[19]提出了一些研究工作。
8. 一般来说，这些方法主要包括两个步骤，即:采用全卷积网络(FCN)等分割网络生成文本预测图，采用几何方法生成倾斜建议
9. 然而，前提分割通常是耗时的。
10. 此外，一些系统需要几个后处理步骤来生成具有所需方向的最终文本区域提案，因此不如直接基于检测网络的提案有效率
11. 本文提出了一种基于旋转的文本检测方法和一种面向任意文本检测的端到端文本检测系统。
12. 特别地，方向被合并以便检测系统可以产生任意方向的建议
13. 图1给出了以前基于水平的方法与我们的方法的比较。
14. 提出了一种基于文本倾斜角信息的旋转区域建议网络(RRPN)
15. 然后利用角度信息进行边界框回归，使建议更准确地适合文本区域
16. 提出了旋转感兴趣区域(RRoI)池层，将任意方向的方案投影到特征图中
17. 最后，利用两层网络将区域划分为文本区域和背景区域。
18. 本文的主要贡献包括:
19. 与以往基于分段的框架不同，我们的框架能够使用基于区域提议的方法预测文本行的方向;因此，该方案能较好地拟合文本区域，且范围文本区域易于校正，更便于文本阅读。
20. 基于区域的提案体系结构[20]引入了RRoI池层和旋转提案学习等新组件，与基于分段的文本检测系统相比，保证了文本检测的计算效率。
21. 我们还提出了改进任意方向区域提案的新策略，以提高任意方向文本检测的性能。
22. 我们将框架应用于三个真实的文本检测数据集，即， MSRA-TD500 [21]， ICDAR2013[22]和ICDAR2015[23]，发现与之前的方法相比，该方法更加准确，效率显著提高
23. 本文的其余部分组织如下。第二部分介绍了场景文本检测的背景及相关工作。
24. 第三节简要审查了横向区域建议办法
25. 第四部分详细讨论了我们的框架
26. 在第五部分，对三个数据集进行了定量研究
27. 我们在第六节总结我们的工作。

## Related Work

1. 近几十年来，人们一直在研究野外阅读文本;综合调查可以在[24]-[27]中找到。
2. 设计了基于滑动窗口、连接组件和自底向上策略的文本检测方法。
3. 基于滑动窗口的方法[7]，[10]，[28]-[30]倾向于使用一个固定大小的滑动窗口来滑动文本区域，找到最可能包含文本的区域
4. 为了考虑更精确的文本样式，[10]、[31]对滑动窗口方法应用了多种比例和比例
5. 然而，滑动窗口过程会导致较大的计算开销和效率低下。
6. 具有代表性的基于连接组件的方法如笔画宽度变换(SWT)[32]和最大稳定极值区域(MSER)[33]在ICDAR 2011年[34]和ICDAR 2013年[22]鲁棒文本检测竞赛中表现优异。
7. 它们主要通过边缘检测或极值区域提取来检测图像的边缘和像素点，然后将子mser组件组合成一个单词或文本行区域
8. 这些方法在涉及多个连通字符、分割笔画字符和不均匀光照[35]的困难情况下，其性能受到限制。
9. 野外场景文本通常与现实应用程序中的任何方向对齐，并且需要实现任意方向的方法
10. 例如,[36]使用相互对称和对称梯度向量识别文本像素级候选无论方向,包括曲线从自然场景图像,和[37]设计一个Canny文本探测器通过图像边缘和文本之间的相似度检测和执行文本定位文本边缘像素。
11. 最近，基于卷积网络的文本检测方法被提出，例如text - cnn [38]
12. 最近，提出了基于卷积网络的文本检测方法，如text -CNN[38]，首先使用优化的MSER检测器找到文本的近似区域，然后将区域特征发送到基于字符的水平文本CNN分类器中，进一步识别字符区域
13. 此外，在Yao等人开发的[18]分割模型中采用了方向因子。
14. 他们的模型旨在通过明确的文本分割方式预测更准确的方向，并在ICDAR2013[22]、ICDAR2015[23]和MSRA-TD500[21]基准测试中取得了优异的结果
15. 一种类似于文本检测的技术是通用对象检测。
16. 如果大大减少提案的数目，就可以加快检测进程
17. 区域建议方法有很多种，比如Edge Boxes[39]Selective Search[40], 和 Region Proposal Networks (RPNs) [20].
18. 例如，Jaderberg等人的[41]扩展了区域建议方法，并应用边缘框方法[39]来执行文本检测
19. 他们的文本检测系统在多个文本检测基准上取得了优异的效果
20. 连接体文本提议网络(CTPN)[42]也是一种基于检测的场景文本检测框架。
21. 它利用LSTM中CNN网络的图像特征来预测文本区域，并生成鲁棒的建议。
22. 本工作的灵感来自于RPN检测管道，基于稠密建议的方法用于检测，RoI池操作用于进一步加速检测管道
23. 基于RPN的检测管道广泛应用于各种计算机视觉应用[43]-[45]。
24. 这一思想也与空间变压器网络(STN)[46]的思想相似，即，神经网络模型可以通过学习图像的仿射变换矩阵对图像进行校正
25. 在此，我们尝试通过注入角度信息将模型扩展到面向多方向的文本检测
26. 也许与我们最相关的工作是[43]，作者提出了一种incepepi - rpn，并进一步对文本检测进行了特定的优化，以适应文本检测
27. 我们将旋转因素纳入区域提案网络，使其能够产生以任意方向的提案
28. 我们还将RoI池层扩展到旋转RoI池层，并在我们的框架中应用角度回归进行校正，最终取得了很好的效果

## Horizontal Region Proposal

1. 我们首先简要回顾一下RPN[20]。
2. 如前一节所述，RPN能够进一步加速提案生成的过程
3. 采用VGG-16[47]部分作为可共享层，在最后一个卷积层的feature map上滑动生成水平区域的proposal。
4. 每个滑动窗口的特性提取被送入两个兄弟层(box-regression (reg)层和boxclassification (cls)层),用4 k每提议坐标(4)输出从reg层代表坐标和2 k(2分/建议)分数从cls层k每个滑动位置的锚。
5. 为了使对象适应不同的尺寸，RPN使用两个参数来控制锚的尺寸和形状，即，比例和长宽比。
6. scale参数决定锚的大小，纵横比控制锚盒的宽高比。
7. 在[20]中，作者将一个通用对象检测任务的比例设置为8、16和32，比例设置为1:1、1:2和2:1。
8. 这种锚点选择策略可以覆盖几乎所有自然物体的形状，并使提案总数保持在较低水平。
9. 然而，在文本检测任务中，尤其是对于场景图像，文本往往呈现出不同方向的非自然形状;RPN生成的基于轴向对齐的方案对场景文本检测不具有鲁棒性
10. 为了使网络对文本检测具有更强的鲁棒性，并保持文本检测的效率，我们认为有必要构建一个检测框架，用区域建议对旋转信息进行编码

## Approach

1. 现在我们详细阐述了基于旋转的框架的构建;结构如图2所示。

2. 我们在框架的前面使用了VGG-16[47]的卷积层，它由两个同级分支共享，即， RRPN和最后一个卷积层的特征映射的克隆。

3. RRPN为文本实例生成面向任意的建议，并进一步对建议执行边界框回归，以更好地适应文本实例。

4. 从RRPN分支出来的同级层是RRPN的分类层(cls)和回归层(reg)

5. 这两层的输出是来自cls的分数和来自reg的建议信息，计算和汇总它们的损失，形成一个多任务损失

6. 然后，RRoI池层作为一个最大池层，通过将面向任意性的文本建议从RRPN投射到特征图上

7. 最后，利用两个完全连通的层构成分类器，将具有RRoI特征的区域分为文本区域和背景区域

   ### Rotated Bounding Box Representation

   1. 在训练阶段,一个文本区域的地面真值被表示为旋转边界盒5元组(x, y, h, w,θ)。

   2. 坐标(x, y)表示包围框的几何中心。

   3. 高度h设置为边界框的短边，宽度w设置为长边

   4. 方向θ角从x轴的正方向旋转的方向平行于长边边界框

   5. 由于场景文本检测的特殊能力，读取的方向及其相反方向不影响检测区域

   6. 在这里,我们只是保持取向参数θ,它涵盖了角的一半空间。

   7. 假设的方向旋转盒子是θ;存在一个且只有一个整数k确保θ+ kπ区间内(−π/ 4,3π/ 4),和我们更新θ+ kπ作为θ。

   8. 元组(x, y, h, w,θ)表示有三个好处

   9. 首先，很容易计算两个不同旋转框之间的角度差

   10. 其次，这是每个旋转边界框的角度回归的友好的旋转表示。

   11. 第三，与传统的旋转边界框的8点表示(x1、y1、x2、y2、x3、y3、x4、y4)相比，该表示方法可以方便地计算训练图像旋转后的新ground truth

   12. 假设给定图像的大小是$I_H×I_W$和原始文本区域表示为(x, y, h, w,θ)。

   13. 如果我们把图像围绕中心旋转一个角度α∈(0,2π),中心锚可以计算:
       $$
       [x^{'},y^{'},1 ]^T = T(I_W/2,I_H/2 )R(\alpha)T(-I_W/2,-I_H/2 )[x,y,1 ]^T
       $$
       T和R分别为平移矩阵和旋转矩阵，
       $$
       T(\delta_x, \delta_y) = \left[
       	\begin{matrix}
       	1 & 0 & \delta_x\\
       	0 & 1 & \delta_y\\
       	0 & 0 & 1\\
       	\end{matrix}
       	\right]
       $$

       $$
       R(\alpha) =\left[
       	\begin{matrix}
       	cos \alpha & sin\alpha & 0\\
       	-sin\alpha & cos\alpha & 0\\
       	0 & 0 & 1\\
       	\end{matrix}
       	\right]
       $$

       

   14. 宽度$w^{'}​$和高度$h^{'}​$的边界框旋转不变,方向是$θ=θ+α+ kπ  \quad  (θ∈(−π/ 4,3π/ 4))​$。

   15. 在训练过程中，我们使用了这种图像旋转策略来增强数据。

   ### Rotation Anchors

   1. 传统的锚点使用尺度和长宽比参数，无法实现文本的野外检测。
   2. 因此，我们对旋转锚(r -anchor)进行了多次调整设计
   3. 首先，添加一个方向参数来控制提案的方向。
   4. 六种不同的取向。,−π/ 6 0,π/ 6,π/ 3π/ 2和2π/ 3,这是方向覆盖率和计算效率之间的权衡
   5. 其次，由于文本区域通常具有特殊的形状，所以将长宽比更改为1:2、1:5和1:8，以覆盖广泛的文本行。
   6. 此外，保留了8、16和32的比例尺
   7. 锚定策略如图3所示
   8. 我们的数据表示一步后,会从R-anchors生成一个 提议，有5变量(x, y, h, w,θ)。
   9. 对于feature map上的每个点，生成54个r -anchor(6个方向，3个纵横比，3个比例尺)，对于reg层生成270个输出(5×54)，对于cls层在每个滑动位置生成108个score输出(2×54)
   10. 然后用RRPN对feature map进行滑动，生成H×W×54个锚点，feature map的宽度为W，高度为H。

   ### Learning of Rotated Proposal

   1. 在生成r -anchor的过程中，需要对r -anchor进行采样策略来进行网络学习。

   2. 首先，我们将相交-过并集(IoU)重叠定义为ground truth的倾斜矩形与R-anchor之间的重叠

   3. 然后,positive R-anchors特性如下:(i)最高的IoU overlap或IoU大于0.7对ground truth,和(2)一个转角对ground truth小于π/ 12。

   4. Negative R-anchors特点是以下几点:(i)一个IoU低于0.3,或(2)一个IoU大于0.7,但与一个转角与ground trouth大于π/ 12。

   5. 在培训过程中不使用未被选择为阳性或阴性的区域。

   6. 我们的loss function采用多任务损失[48]的形式，定义为
      $$
      L(p,l,v^{*}, v) = L_{cls}(p,l)+\lambda lLeg_{reg}(v^{*}, v)
      $$
      l是类标签指示器，（文本的l=1, 背景的l=0,背景没有回归），参数$p = (p_0, p_1)$是softmax计算的类的概率，$v = (v_x, v_y, v_h, v_w, v_{\theta})$是预测的文本标签的元组，$v = (v_x^*, v_y^*, v_h^*, v_w^*, v_{\theta}^*)$表示ground truth

   7. 两项之间的权衡是由平衡控制参数λ。

   8. 我们将l类的分类损失定义为:
      $$
      L_cls(p,l)=-logp_l
      $$

   9. 对于边界盒回归，忽略背景roi，文本roi采用平滑l1损失:
      $$
      L_{reg}(v^*, v)=\sum_{i∈\{x,y,h,w,\theta \}}smooth_{L1}(v_i^* - v_i)
      $$

      $$
      smooth_{L1}(x) = 
      \begin{cases}
      0.5x^2 & \text {if |x|<1}\\
      |x|-0.5 & \text {otherwise}
      \end{cases}
      $$

      

   10. 尺度不变参数化元组v和$v^*$被求出如下:
       $$
       v_x = \frac {x-x_a} {w_a}, \quad  v_y = \frac {y-y_a} {h_a} \\
       v_h=log \frac {h}{h_a}, \quad v_w = log \frac {w}{w_a}, \quad v_\theta=\theta \Theta\theta_a \\
       v_x^* = \frac {x^*-x_a} {w_a}, \quad  v_y^* = \frac {y^*-y_a} {h_a} \\
       v_h^*=log \frac {h^*}{h_a}, \quad v_w^* = log \frac {w^*}{w_a}, \quad v_\theta^*=\theta^* \Theta\theta_a \\
       $$
       $x$,$x_a$,$x^*$分别是预测框，anchor，和ground truth;$y,h,w,\theta$都是一样的

   11. 运算$a \Theta b = a-b+k\pi$， k∈Z保证$a \Theta b∈[-\pi/4 , 3\pi /4) $

   12. 如前一节所述,我们给R-anchors固定范围内的方向(−π/ 4,3π/ 4),每6方向可以适应ground truth的交会角小于π/ 12。

   13. 因此，每个R-anchor都有它的拟合范围，我们称之为它的拟合域(fit domain)

   14. 当ground truth的方向位于r -anchor的拟合域中时，该r -anchor极有可能是ground truth的正样本

   15. 结果,6个方向的拟合域划分的角度范围(−π/ 4,3π/ 4)6相等的部分。

   16. 因此，任何方向的ground truth都可以用合适的拟合域的r锚进行拟合

   17. 图4显示了回归项的效用比较

   18. 我们可以观察到邻域内区域的取向是相似的

   19. 为了验证网络学习文本区域方向的能力，我们将中间结果可视化，如图5所示。

   20. 对于输入图像，将不同迭代后的RRPN训练特征映射可视化

   21. feature map上的短白线表示对文本实例响应最高的R-anchor

   22. 短线的方向是这个R-anchor的方向，短线的长度表示置信度。

   23. 我们可以看到，feature map中较亮的区域集中在文本区域，而经过150,000次迭代后，另一个区域变得较暗

   24. 此外，随着迭代次数的增加，区域的方向更接近文本实例的方向。

   ### Accurate Proposal Refinement

   1. 倾斜IoU计算:旋转建议可以在任何方向生成
   2. 因此，对轴向对齐的提案的IoU计算可能会导致倾斜的交互式提案的IoU不准确，从而进一步破坏提案学习
   3. 如算法1所示，我们设计了一个考虑三角形[49]的斜IoU计算实现;6展示了几何原理
   4. 给定一组倾斜矩形R1，…Rn，我们的目标是计算每一对Ri Rj的IoU。
   5. 第一步是生成Ri与Rj的交点集PSet(算法1中的第4-7行)。
   6. 然后计算PSet的交点面积(算法1中的第8-10行)。
   7. 将PSet中的点按其在图像中的位置按逆时针方向进行排序，并根据排序后的点生成凸多边形
   8. 三角测量,我们可以获得三角形集合(例如,{ΔAIJΔAJC,ΔACK,ΔAKL}在图6 (b))。
   9. 多边形的面积是三角形面积的和。
   10. 最后，计算IoU值。
   11. Skew Non-Maximum Suppression (Skew-NMS):
   12. 传统的NMS只考虑IoU因素(例如，IoU阈值为0.7)，但对于以任意方向的提案是不够的。
   13. 例如,anchor 纵横比1:8和一个角度差π/ 12的IoU为0.31,小于0.7;
   14. 然而，它可以被认为是一个积极的样本
   15. 因此，Skew-NMS包括两个阶段:(i)对于IoU大于0.7的提案，保留最高IoU;(2)如果所有建议有一个借据在[0.3,0.7],保持最小角度差的提议对ground truth(角差应小于π/12)。

   ### RRoI Pooling Layer

   1. 针对Fast-RCNN [48]， RoI池层从每个提案的feature map中提取一个固定长度的feature vector。
   2. 每个特征向量被输入到完全连接的层中，最终分支到同级的cls和reg层中，输出是输入图像中对象的预测定位和类。
   3. 由于图像的特征映射只需要对每幅图像计算一次，而不需要对生成的每一个建议都计算一次，因此加速了目标检测框架
   4. RoI池层使用max池将任何有效RoI内的特征转化为一个固定空间范围hr×wr的小feature map，其中hr和wr是独立于任何RoI的层超参数。
   5. 对于任意文本检测任务，传统的RoI池层只能处理轴向对齐的提案。
   6. 因此，我们提出了旋转RoI (RRoI)池层来调整由RRPNs生成的面向任意性的提案
   7. 我们首先将RRoI层超参数设置为Hr和Wr
   8. (略。。。。。。)

   

   ### Experiments

   1. 我们在三个流行的文本检测基准上评估了基于旋转的框架:MSRA-TD500[21]、ICDAR2015[23]和ICDAR2013[22]。

   2. 我们遵循这些基准的评估协议。

   3. MSRA-TD500数据集包含300张训练图像和200张测试图像

   4. 图像的标注由每个文本实例的位置和方向两部分组成，该基准可用于评价面向多个文本实例的文本检测性能

   5. 由于MSRA-TD500的数据集相对较小，因此其实验旨在利用其他设置

   6. .ICDAR2015发布用于ICDAR2015健壮阅读大赛附带场景文本挑战赛文本本地化(Task 4.1);总共有1500张图片

   7. 与以往的ICDAR鲁棒性阅读比赛不同，文本实例注释有四个顶点，形成一个不规则的四边形边界框，其中包含方向信息

   8. 我们粗略地生成一个倾斜矩形来适应四边形及其方向

   9. ICDAR2013数据集来自ICDAR2013健壮阅读大赛。

   10. 其中用于训练的自然图像229张，用于测试的自然图像233张。

   11. 这个数据集中的所有文本实例都是水平对齐的，我们在这个水平基准上进行了实验，以确定我们的方法对特定方向的适应性。

   12. 实现细节:我们的网络是通过预先训练一个用于ImageNet分类[47]的模型初始化的。

   13. 在前200,000次迭代中使用10−3的学习率，在后100,000次迭代中使用10−4的学习率来更新网络的权值，权值衰减为5×10−4，动量为0.9。

   14. 我们使用随机角度的图像旋转来进行数据增强，因为当使用增强时，它们的效率和测量值都得到了提高(见表I)。

   15. 由于我们不同的R-anchor策略，每幅图像的提案总数几乎是之前fast - rcnn等方法的6倍。

   16. 为了保证有效的检测，我们过滤了r -anchor来去除那些通过图像边界的点。

   17. 因此，无论是在培训阶段还是测试阶段，我们的系统速度都与之前的工作速度相似;表vi -左侧给出了与MSRA-TD500上最先进方法的比较。

   18. 表II显示了我们提出的框架的运行速度，以及在基线设置和边框填充的情况下原始的fast - rcnn的运行速度。

   19. 我们可以观察到，我们的方法花费的时间是fast - rcnn方法的两倍。

       #### Ablation Study

       1. 我们首先对较小的数据集进行消融研究，即,MSRA-TD500。

       2. 基线系统使用来自MSRA-TD500训练集的300幅图像进行训练;输入图像的大小被调整，长边为1000像素。

       3. 评价结果的准确率为57.4%，召回率为54.5%，F-measure为55.9%，与原来的fast - rcnn相比，性能有了很大的提高，P、R和F分别为38.7%、30.4%和34.0%

       4. 我们将旋转区域和水平区域的方案进行了比较，一些检测结果如图8所示。

       5. 基于旋转的方法能够在较小的背景面积下实现精确的检测，表明了该方法的有效性

       6. 对基线结果的进一步分析给我们提供了以下见解:

       7. (i)难以察觉图像中的困难情况(例如光线模糊和不均匀);

       8. (ii)无法正确地检测到一些非常小的文本实例，导致性能方面的大量召回损失

       9. (iii)极长的文本行，即，边界框的高宽比大于1:10，不能正确检测到，常常被分成几个较短的方案;

       10. 因此，通过对MSRA-TD500的评估，所有的提案都成为了误检的实例(一些失败的检测实例如图9所示)。

       11. 测试了基线方法的一些备选策略和设置;表三列有摘要。

       12. **Context of the Text Region:**文本区域的上下文:合并上下文信息已被证明对于一般对象检测任务(例如[50])是有用的，我们想知道它是否可以促进文本检测系统

       13. 我们保留了旋转包围框的中心及其方向，并将宽度和高度都放大了1.X倍在数据预处理步骤。

       14. 在测试阶段，我们为每个提案划分扩展。

       15. 如表四所示，所有实验F-measure都有明显的增加。

       16. 原因可能是随着包围框变大，文本实例的上下文信息会更多，关于方向的信息可以更好地捕获。

       17. 因此，可以更准确地预测各项建议的方向。

       18. **Training Dataset Enlargement**扩大训练数据集：我们将[51]作为一个额外的数据集，并从两个数据集中形成一个包含700张图像的培训集。

       19. 所有的测量结果都有了显著的改善，F-measure为60.8%，表明该网络在处理噪声输入时训练得更好，鲁棒性更强

       20. **Border Padding**边缘填充：使用我们的过滤策略，大部分的边界突破r-anchor被消除

       21. 但是，当包围框旋转一定角度时，它仍然可能超出图像边界，特别是当我们放大文本区域以获取上下文信息时

       22. 因此，我们为每一方设置了0.25倍的边界填充，以保留更积极的提议

       23. 实验表明，在图像中添加边框填充可以提高检测效果

       24. 边界填充使我们的方法的计算量增加了大约5%(表II)。

       25. 此外，将边界填充与文本区域的扩大以及训练数据集相结合，F-measure得到了63.3%的进一步改进。

       26. **Scale Jittering**比例抖动：在这两个训练数据集中仍然有一些小的文本区域，我们希望提高我们的系统的鲁棒性

       27. 一种方法是将输入图像缩放到固定的较大尺寸，另一种方法是执行缩放抖动，即，在将图像发送到网络之前，用随机大小的长边重新缩放。图10显示，长边为1300像素的输入优于其他固定设置(精度:71.1%，召回率:65.3%，F-measure: 68.1%)。

       28. 当随机长度小于1300像素的尺度抖动应用时，与不抖动的实验结果相比，取得了较好的效果

       #### Performance on Benchmarks

       1. **MSRA-TD500**：我们使用消融研究中最好的设置。
       2. MSRA-TD500的注释更喜欢标记整个文本行的区域
       3. 因此，文本行长度没有固定的范围，有时非常长
       4. 但是，R-anchor的比率是固定的，可能不够大，不能覆盖所有的长度，这将导致单个文本区域出现多个短边框结果
       5. 为了解决这个超长文本行问题，将多个短检测段链接到一个更精细的提案中，从而加入了一个后处理步骤，如算法3所示。
       6. 我们还对fast - rcnn[20]进行了后处理和V-A部分提出的策略的实验;结果的准确率为42.7%，召回率为37.6%，F-measure为40.0%。
       7. 比较验证了使用基于旋转的框架来实现更健壮的文本检测器是必要的。
       8. 请注意，后处理应用于文本行检测基准，即，只有MSRA-TD500，而我们并没有将该算法应用到ICDAR的基准测试中。
       9. MSRA-TD500的结果(RRPN)显示在表VI的最左边一列。
       10. **ICDAR2015**:我们使用与MSRATD500相同的策略在ICDAR2015基准上训练基线实验。
       11. 评价结果的精密度为45.42%，召回率为72.56%，F-measure为55.87%。
       12. 这两个数据集有一些不同之处
       13. MSRA-TD500倾向于提供文本行基本事实，而ICDAR提供单词级注释。
       14. 因此，我们的方法的精度低于其他方法，实现相同的F-measure
       15. 这个问题可能源于三个方面。
       16. 首先，一些附带的文本区域仍然太小，我们的检测器无法找到。
       17. 其次，ICDAR2015训练集中存在一些小的不可读文本实例(标记为“###”)，这可能导致对类似文本的实例的错误检测.
       18. 最后，与之前的方法相比，如[17]、[18]、[42]，我们的训练集是不够的(仅包含1000张图像)。
       19. 基于ICDAR2015训练集得到的部分检测结果如图12所示。
       20. 使用交叉验证的不可读文本实例比例对训练集的影响。
       21. 为了解决小文本区域的问题，我们创建了一个更大的规模，在发送到网络之前，随机大小小于1,700像素的长边的图像修补程序进行抖动
       22. 我们还通过从训练集中随机删除小的不可读文本实例来检查这些实例的影响。图13显示了测量值的曲线
       23. 召回率保持不变，即，约为72%-73%，除非我们删除所有不可读的实例，而精度随比例显著增加
       24. 因此，我们随机删除训练集中80%的不可读文本实例，并保留整个测试集。
       25. 为了进一步改进我们的检测系统，我们加入了一些用于培训的文本数据集，即， ICDAR2013 [22]， ICDAR2003[52]和SVT[7]。
       26. 如表五所示，不同方法的训练图像具有相同的数量级，我们的训练效果更好。
       27. **ICDAR2013**:为了检验我们的方法的适应性，我们还在基于水平的ICDAR2013基准上进行了实验。
       28. 我们重用了为ICDAR2015训练的模型，并且5元组旋转建议适合于水平对齐的矩形。
       29. 根据ICDAR 2013评估方案，结果的准确率为90.22%，召回率为71.89%，F-measure为80.02%。
       30. 如表六所示，与fast - rcnn相比，改进了7%，验证了我们的检测框架对旋转因子的鲁棒性。

       #### More Results

       1. 我们的方法与最先进的方法的实验结果比较如表六所示
       2. 由于RRPN模型分别针对MSRA-TD500和ICDAR进行训练，我们还针对所有训练集训练了一个统一的模型(RRPN*)来考虑泛化问题
       3. 三个数据集上RRPN和RRPN*的精确回忆曲线如图15所示。
       4. 对于MSRA-TD500数据集，我们的RRPN的性能达到了与最先进的方法(如[18]和[17])相同的水平。
       5. 当我们的系统实现文本检测时，它比其他系统更高效，只需要处理0.3 s每幅测试图像。
       6. 对于ICDAR基准测试，相对于已发表的工作，性能的显著提高证实了使用旋转区域建议和旋转RoI用于文本检测任务的有效性。
       7. 最近开发的DeepText[43]也是一种基于检测的方法，但它基于Inception-RPN结构。
       8. 我们的方法和DeepText都是基于ICDAR2013基准进行评估的
       9. 表VI中的评价结果和图14中的检测示例表明，我们的方法在不同的评价度量方面表现得更好
       10. 我们相信我们的基于旋转的框架也是对incepe - rpn结构的补充，因为它们都关注不同层次的信息。
       11. 在基准上得到的一些检测结果如图11所示，我们已经发布了代码，并为以后的研究提供了训练好的模型

## Conclusion

1. 本文介绍了一种基于旋转的任意文本检测框架。
2. 利用网络中较高卷积层的文本区域方向角信息生成倾斜矩形建议，实现了对多方向文本的检测
3. 设计了一种新的RRoI池层，并将其应用于旋转roi。
4. 通过与MSRA-TD500、ICDAR2013和ICDAR2015上最先进的文本检测方法的实验比较，证明了我们提出的RRPN和RRoI用于文本检测任务的有效性和效率。

