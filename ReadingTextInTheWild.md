1. 我们在实验中暴露了管道各个部分的性能，表明了随着模型的复杂程度和高阶信息的加入，我们可以保持初始方案阶段的高召回率，同时逐步提高精度。
2. 检测阶段的召回率明显高于以往的文本检测方法，单词识别阶段的正确率高于所有之前的方法。
3. 结果是一个端到端的文本识别系统，在很大程度上超过了以前的所有方法。
4. 我们将会对注释任务进行演示（本地化和识别图像中的文本）在大量标准文本识别数据集，以及标准数据集的检索场景（检索包含查询字符串文本的图像的排序列表）。
5. 另外，我们的框架在一个真实世界的应用中得到进一步的演示，这个应用程序为用户给的查询文本即时搜索数千小时的归档新闻片段。
6. 下一个部分概述了我们的pipeline。然后我们在第3部分中回顾了一些相关的工作，第4-7节介绍了pipeline的各个阶段。我们详细地测试了我们pipeline的所有元素，包括数据集和实验设置的细节。最后，第9节是概括和总结。
7. 我的单词识别框架最初是作为技术报告出现在NIPS2014深度学习和表示学习（Representation Learning）研讨会上，以及其他一些非基于字典的变体出现的。
8. 为了识别和排名提案，我们同时训练一个很大的CNN在整个提案区域来执行文字识别，不同于以往的基于字符分类器的系统。
9. 这个网络用一个人工文本生成引擎来生成训练数据，不需要人为打上标签。
10. 分析我们管道的各个阶段，我们自始至终展示了最先进的性能。
11. 我们通过对许多E2E的文本定位基准和基于文本的图像检测数据集进行了严格的实验，比起所有以前的方法展示了巨大的提升。
12. 最后，我们展示了一个我们文字识别系统的实际应用，允许通过文本查询立即检索数千小时的新闻片段



## Introduction

1. 自然图像中文本的自动检测和识别——文本识别，是视觉理解中的一个很重要的挑战。
2. 文本作为语言的物理化身，是保存和交流信息的基础工具。
3. 现代世界上很多东西被设计为通过标签和文本线索来解释，所以文本散步在很多图片和视频中。
4. 通过文本识别，视觉媒体中的语义内容的很重要的部分可以被解码和使用，比如，可以了解、注解和检索每天生成的数十亿消费者的照片。
5. 传统的，文本定位主要集中在文档图像上，OCR技术非常适合与数字化平面的纸质文档。
6. 然而，当这些技术应用到自然环境图片时，这些文档OCR技术失败了，因为他们被调整为主要用在黑白的，基于行的打印文档环境中。
7. 这些出现在自然环境图片中的文本在外表和布局上变化很大，被大量的字体和样式绘制，存在着不一致的灯光、遮挡、方向、噪声，此外，背景对象的存在会导致虚假的假阳性检测。
8. 这里将文本定位作为一个独立的，比文档OCR更加有挑战性性的问题
9. 在过去的十年中，计算机视觉技术强有力的增长和图像的大量产生使得文本定位技术快速发展。
10. 为了更有效率地执行文本识别，大多数方法遵循将任务一分为二的直观过程：文本检测和单词识别。
11. 文本识别涉及到生成候选字符或者文字区域检测，文字识别接受这些建议并推断所描述的词。
12. 在这篇paper中，我们提出了文本定位的方法，作为其中的一部分， 做了一些关键的贡献。
13. 我们的主要贡献是一种**新颖的文本识别方式**——这是一种以深度CNN的模式，他以全词图像作为网络的输入。
14. 从图像中逐渐汇集证据，在一个巨大的字典中进行分类，比如本文在一个90k的字典中评估。
15. 非常显著地，我们的模型**纯粹基于合成的数据**，没有人为打标签的成本。
16. 我们也提出一种**增量式学习方法**来成功训练这么一个有大量类的模型。
17. 我们的识别框架异常强大，在没有使用任何真实世界标记的训练数据的情况下，实际上在现实场景的文本识别上大大超过了以前的水平。
18. 我们的第二个贡献是一种新颖的文本检测策略：使用**快速区域建议方法**执行单词检测
19. 我们使用**对象无关区域提议方式**和滑动窗口检测器的组合。
20. 这提供了非常高的单个词的bounding boxes的召回率（recall）,导致了在ICDAR 2003和有可管理的提议数量的Street View文本数据集上有98%的词召回率。
21. 假阳性（False-positive）候选词的bounding boxes使用更强的随机森林分类器（random forest classifier）进行过滤，其余的建议调整使用一个CNN训练回归bounding box坐标。
22. 我们的第三个贡献时将我们的pipeline应用于视频中文本的大规模检索。
23. 在极短的时间内，我们可以从一个大型的包含用户给定文本查询的可视化呈现的语料库中检索图像和视频，而且有非常高的精度（precision）.



## Overview of the Approach

1. 我们的方法分为以下几个阶段：**生成文本框提案、提案过滤和调整、文本识别和具体任务的最终合并**。整个过程如图所示。

2. 我们的过程大致遵循检测/识别分离——单词检测阶段之后是一个单词识别阶段

3. 然而，这两个阶段并不是完全不同的，当我们使用单词识别中获得的信息进行归并，最后对检测结果进行排序，从而形成一个更强大的整体性的文本识别系统。

4. 我们pipeline的检测阶段是基于**弱而快速的检测方法**来生成字框建议

5. 这借鉴了**Girshick et al.(2014)的R-CNN对象检测框架**的成功，该框架将区域提案映射到一个固定的大小，用于CNN识别

6. 区域建议的使用避免了评估一个穷尽多尺度的昂贵分类器，多方面比滑动窗口的搜索的计算复杂度

7. 我们使用了**Edge Box proposals(Zitnick and Dollar2014)**和训练有素的**aggregate channel features detector(Dollar et al. 2014)**(聚合通道特征检测器)生成候选词包围框。

8. 由于大量的假阳性（false-positive）建议，我们使用一个随机森林分类器来过滤建议的数量到一个可管理的规模——这是一个比起这些发现再建议算法更强大的分类器。

9. 最后，受**DPM**和**R-CNN**中bounding box回归成功的启发，我们从提案算法的种子中回归出更准确的bounding box，大大提高了阳性检测（positive detection）和**groundtruth**的平均重叠率（average overlap ratio）

10. 然而，与Felzenszwalb等人的线性回归函数不同，我们特别地训练了一个CNN来回归。我们将在每个部分中讨论这些设计选择。

11. 框架的第二个阶段为检测阶段生成的每个提案生成一个文本识别结果。

12. 我们采用全词识别方法，提供单词的整个剪裁区域作为输入输入到CNN。

13. 我们提出一个字典模型，该模型将识别任务作为一种跨90k可能性单词的字典的多路分类任务。

14. 由于这种规模的分类任务需要大量的训练数据，这个模型都是纯粹使用合成数据训练的。

15. 我们的合成数据引擎能够呈现足够逼真和可变的单词图像样本，模型在这些转换到现实世界的单词图像领域数据上训练，提供最先进的识别正确率。

16. 最后，我们使用从识别中收集到的信息，对检测结果进行多轮非极大抑制（non-maximal suppression）和bounding box回归。

    ## Related Work

    ### Text Detection Methods

    1. 文本检测方法解决了标准文本检测管道[11]的第一个任务:在自然场景图像中生成单词的分割或包围框。
    2. 检测噪声和杂乱图像中的单词实例是一项非常重要的任务，为了解决这个问题开发的方法是基于字符区域（character regions）或者滑动窗口(sliding windows)的。
    3. 字符区域方法的致力于将像素分割为字符，然后将字符分组为单词。
    4. Epshtein等人通过笔划宽度变化（SWT）找到输入图像中笔划宽度恒定的区域——两条平行边之间的距离
    5. 直观上，字符是笔画宽度相近的区域，所以像素聚类形成字符，字符基于几何启发式组合组成单词
    6. 在[44]这篇论文中，Neumann和Matas重新讨论了字符表示为笔画的概念，并使用梯度过滤器代替SWT检测有方向的笔画。比起笔划宽度恒定的区域，Neumann和matas使用极值区域作为字符区域。
    7. 黄等人，结合了一个强大的CNN分类器来有效地修剪极值区域的树，从而减少假阳性（False-positive）检测，扩展了极大稳定极值区域的使用。
    8. 滑动窗口方法（sliding windows method）将文本检测作为一种经典的目标检测任务。
    9. Wang等人使用随机蕨类分类器对滑动窗口场景中的HOG特征进行训练，寻找图像中的字符。
    10. 这些词是用小的固定词汇的象形结构框架组合形成的词。
    11. Wang&Wu等人证明了经字符分类训练的CNN可以作为有效的投影滑动窗口分类器。
    12. 在我们的一些早期工作中，我们使用CNN训练一个用于滑动窗口评估的文本/非文本分类器来做文本检测，并且也使用字符和双字母组合的CNN来进行文本识别。
    13. 我们发现，对于不同的分类任务，似乎用跨越所有CNN的特征共享比单独训练每个分类器能训练出更为强大的分类器用于文本检测
    14. 与以前的方法不同，我们的框架以低精度（low-precision），高召回率（high-recall）的方式运行——而不时使用单个单词区域建议，我们通过管道的几个阶段携带了足够多的候选项。
    15. 我们使用高召回率的区域建议方法和过滤阶段进一步完善这些。
    16. 事实上，我们的“检测方法”是在对每一个剩余的提案进行全文识别后才完成的，然后我们识别阶段的输出对提案进行归并和排序，给出最终的检测结果，并于它们的识别结果一起完成。

    ### Text Recognition Methods

    1. 文本识别的目的是得到单个单词的裁剪图像，并且识别这个单词图像。
    2. 虽然之前有很多工作专注于手写或者历史文档识别，但由于文档中没有呈现高度变化前景和背景纹理，这些方法在功能上不能推广到通用场景文本。
    3. 对于**场景文本识别**（scene text recognition）,方法可以分为两组——**基于字符的识别**（character based recognition）和**全字识别**（whole word based recognition）
    4. 基于字符的识别依赖于单个字符分类器对于每个字符进行识别，该分类器集成在单词图像中生成完整的单词识别。
    5. Yao等人2014年的论文中，通过对字符的子块进行聚类，学习了一组中层特征（mid-level features），利用霍夫投票（Hough voting）检测字符，利用作用于strokelet和HOG特征的随机森林分类器识别字符。
    6.  Alsharif and Pineau (2014), Bissacco et al. (2013), Jaderberg et al. (2014), Wang et al. (2012) 等人的做法都是使用CNNs作为字符分类器。
    7. Bissacco et al.(2013)和Alsharif and Pineau(2014)等人通过非监督二值化技术（unsupervised binarization technique）或监督分类器（supervised classifier）将单词图像过度分割成潜在的字符区域。
    8. Alsharif和Pineau(2014)将分段校正（segmentation-correction）和字符识别CNNs与固定词汇的**HMM**复杂结合，生成最终的识别结果。
    9. PhotoOCR系统利用神经网络分类器作用于片段的HOG特征作为评分，利用**波速搜索**（beam search）找到片段的最佳组合。
    10. 波速搜索结合了强 N-gram语言模型，最终的波速搜索方案通过进一步的语言模型和形状模型重新排序。
    11. 我们自己之前的工作使用二进制本文/无文本分类器（text/no-text classifier）,字符分类器，和在单词图像中密集计算的双字母组分类器（bigram classifier）的组合，作为在固定词汇上下文中**Viterbi**评分函数的提示。
    12. 作为一种可选择的词识别方法，其他方法使用全词识别，在进行词分类之前从整个子图像中汇集特征。
    13. Mishra et al.(2012)和Novikova et al.(2012)的作品仍然依赖于显式的字符分类器，但构建了一个图形来推断单词，汇集了完整的单词证据。
    14. Goel et al.(2013)通过对比简单的黑白字体渲染词，利用整词子图像特征（whole word sub-image feature）识别词。
    15. Rodriguez-Serrano et al.(2013)使用聚合的Fisher向量(Perronnin et al. 2010)和一个结构化的SVM框架来创建一个联合的单词图像(joint word-image)和文本嵌入(text embedding)。
    16. Almazan et al.(2014)进一步探讨了单词嵌入的概念，为单词图像和单词字符串的表示创建了一个联合嵌入空间。
    17. 这在Gordo(2014)中得到了扩展，Gordo明确使用字符级别的训练数据来学习中级特征.
    18. 这使得性能与(Bissacco et al. 2013)持平，但只使用了少量的培训数据。
    19. (Goodfellow et al. 2013)在不进行全场景文本识别(full scene text recognition)的情况下，利用具有多个位置敏感字符分类器(multiple position-sensitive character classifier)输出的CNN进行街道编号识别取得了很大的成功.
    20. 该模型被扩展到长达8个字符的CAPTCHA序列，在这些序列中，使用合成问题(生成模型已知)的合成训练数据显示了令人印象深刻的性能。
    21. 相反，我们展示了合成训练数据可以用于真实数据问题(生成模型未知)。
    22. 我们的文本识别方法也遵循一种全词图像方法。
    23. 与Goodfellow et al.(2013)类似，我们将单词图像作为输入到深度CNN，但是我们使用的是字典分类模型。
    24. 识别是通过对整个潜在单词字典进行多路分类来实现的。
    25. 在下面的部分中，我们将描述文本定位管道的每个阶段的细节。
    26. 这些部分按它们在端到端系统中的使用顺序给出。

    ### Proposal Generation

    1. 第一阶段是生成文本边界区域

    2. 这就是单词检测——在一个理想的场景中，我们将能够生成具有高回忆率和高精度的单词边界框，这是通过从每个边界框候选者中提取尽可能多的信息来实现的。

    3. 然而，在实践中，为了降低计算复杂性，需要对精度/召回进行权衡。

    4. 考虑到这一点，我们选择了一个快速、高回忆率的初始阶段，使用计算成本低廉的分类器，并逐渐合并更多的信息和更复杂的模型，通过拒绝导致级联的假阳性检测来提高精度

    5. 为了计算检测场景中的调用和精度，如果边界框与定义的阈值之上的地面真实边界框重叠，则称边界框为真正的正检测。

    6. 区域建议方法虽然从未应用于词的检测，但在一般对象检测中得到了广泛的关注。

    7. 区域建议方法的目的是生成高召回率的目标区域建议，但代价是大量的假阳性检测。

    8. 即使如此，与检测管道后续阶段的滑动窗口评估相比，这仍然大大减少了搜索空间。有效地将区域建议方法视为弱检测器。

    9. 在本研究中，我们结合了两种检测机制的结果——边缘盒区域建议算法(Zitnick and Dollar 2014, section . 4.1)和弱集通道特征检测器(Dollar et al. 2014, section . 4.2)。

       #### Edge Boxed

       1. 我们使用了Zitnick和Dollar(2014)中描述的边盒公式。
       2. 边缘框背后的关键直觉是，由于对象通常是**自包含**(self contained)的，因此由包围框完全包围的轮廓数表示包含对象的框的可能性。
       3. 边缘往往与对象边界相对应，因此，如果边缘包含在边界框中，这意味着对象包含在边界框中，而越过边界框边界的边缘则表明有一个对象不完全包含在边界框中。
       4. 当需要的对象是单词(具有明显边界的字符的集合)时，对象是边界集合的概念尤其正确。
       5. 在Zitnick和Dollar(2014)之后，我们使用结构化边缘检测器(Dollar和Zitnick 2013, 2014)计算边缘响应ap，并对边缘响应进行正交的非最大值抑制，使边缘映射稀疏化。
       6. 根据完全由b包含的边的数量，并由b的周长标准化，给候选边界框b赋值sb。完整的细节可以在Zitnick和Dollar(2014)中找到。
       7. 盒子b以滑动窗口的方式，在多个尺度和纵横比上进行评估，并给出sb的评分。
       8. 最后，按照分数对盒子进行排序，并进行非最大抑制:当一个盒子与另一个得分较高的盒子重叠超过阈值时，则删除该盒子
       9. 这将产生一组单词Be的候选边界框。

       #### Aggregate Channel Feature Detector

       1. 另一种生成候选词边界框建议的方法是使用传统的训练检测器

       2. 我们使用Dollar et al.(2014)的aggregate channel features (ACF) detector framework进行计算，因为他的计算速度

       3. 这是一种基于ACF特征和AdaBoost分类器的传统滑动窗口检测器

       4. 基于ACF的检测器在行人检测和一般目标检测方面效果良好，本文采用了相同的检测框架

       5. 对于每个图像I, 可以计算出一些特征通道 ,比如channel C =Ω(I),其中Ω是信道特征提取功能

       6. 我们使用与Dollar等人(2010)类似的通道:标准化梯度大小、定向梯度直方图(6个通道)和原始灰度输入。

       7. 对每个通道C进行平滑处理，将其划分为块，并对每个块中的像素进行求和和平滑处理，从而得到聚合的通道特征。

       8. ACF特征不是尺度不变的，因此多尺度检测需要提取多个不同尺度的特征，即特征金字塔

       9. 在标准检测管道, 局部比例尺s的channel features通过重采样图像和验算channel features Cs=Ω(Is)得到，Cs是在scale s下的channel features， Is = R(I, s)是s重取样后的图像

       10. 在每一个尺度上重新采样和重新计算特征的计算成本都很高。

       11. 但是，如Dollar等人.(2014, 2010)所示，scale s下的channel features可以通过对不同尺度下的特征重新采样来近似
           $$
           C_s = R(C, s)s^{-\lambda\Omega}
           $$
           lambda和Omega是特别的幂率指数

       12. 因此，快速特征金字塔可以被计算通过计算
           $$
           C_s = \Omega(R(I, s))
           $$
           在每一阶，用单一的scale，(s ∈ {1, 1/2 , 1/4 , . . .})，和在中间的scale中。

           Cs的计算通过
           $$
           Cs = R(C_{s'}, s/s')(s/s')^{-\lambda\Omega}
           $$
           s'∈ {1, 1/2 , 1/4 , . . .}.

       13. 这导致了更快的特征金字塔计算

       14. 滑动窗口分类器是一组弱决策树的集合，使用AdaBoost (Friedman et al. 2000)进行训练，使用集合通道特征

       15. 我们对特征金字塔中聚合通道特征的每个块上的分类器进行评估，并对多个纵横比重复此操作，以考虑不同长度的单词，为每个框给出一个分数。

       16. 基于score的阈值化给出了一组来自检测器的单词建议边界框，Bd.

       17. 我们尝试了一些区域建议算法。有些速度太慢，无法发挥作用(Alexe et al. 2012;Uijlings等2013)。

       18. BING (Cheng et al. 2014)是一种非常快速的方法，在专门针对单词检测进行再训练时，该方法具有良好的recall，但检测overlap较低，整体recall较差

       19. 我们发现Edge Boxes可以提供最好的recall和overlap。

       20. 另外，独立使用的情况下，无论是Edge box还是ACF检测器，recall都不是特别高，分别为92%和70%(实验见第8.2节)，但是当这些方案组合在一起时，召回率达到98%(召回计算为0.5的overlap)。

       21. 相比之下，BING单独使用的recall是86%， 与ACF检测器的联合recall为92%。

       22. 这表明盒子边缘和ACF检测器方法非常互补当我们结合使用时,所以我们最后选择的Bounding Boxes B = { Be∪Bd }。

    ### Filtering & Refinement

    1. 第4节的提案生成阶段生成一组候选边界框B。

    2. 然而，为了实现高召回率，会生成数千个边框，其中大多数都是假阳性的。

    3. 因此，我们的目标是使用一个更强的分类器来进一步过滤这些数字，使其在计算上易于管理，以适应第5.1节所述的更昂贵的全文本识别阶段。

    4. 我们还观察到，许多bounding boxes与groundtruth的重叠程度很低，令人不满意，因此训练一个regressor来改进包围盒的位置，如5.2节所述。

       #### Word Classification

       1. 为了减少假阳性词检测的数量，我们寻找一个分类器来执行词/无词二进制分类。
       2. 为此，我们使用随机森林分类器(Breiman 2001)来处理HOG特征(Felzenszwalb et al. 2010)。
       3. 对于每一个bounding box proposal b∈B我们重新取样区域裁剪的图像一个固定大小和提取HOG的特性,作为结果产生描述符h。
       4. 然后使用随机森林分类器对描述符进行分类，并使用树桩节点进行决策。
       5. 随机森林对每个提案进行分类，低于某个阈值的提案将被拒绝，留下一组经过过滤的边界框bf。

       #### Bounding Box Regression

       1. 虽然我们的提案机制和过滤阶段给出了很高的召回率，但是这些提案的重叠可能会很差。

       2. 对于一般的对象检测，0.5的重叠通常是可以接受的(Everingham et al. 2010)，但是对于准确的文本识别来说，这是不能令人满意的

       3. 当边界框的一条边被准确地预测到，而另一条边却没有预测到的时候，这一点尤其正确。如果完美地计算了边界框的高度，那么边界框的宽度可以是其应有宽度的两倍或一半，并且仍然达到0.5重叠。

       4. 如图2所示。这两个预测的边界框与groundtruth都有0.5个重叠，但是对于文本来说，如果高度计算正确，这相当于只看到单词的一半,因此，在图2底部的例子中，不可能识别出正确的单词。

       5. 请注意，这两个建议都包含文本，因此都不会被word/no-word分类器过滤。

       6. 由于有大量的区域建议，我们希望会有一些建议与groundtruth的重叠达到令人满意的程度，但是我们可以通过明确地细化所提议的边界框的坐标来鼓励这一点——我们是在回归框架中这样做的。

       7. 我们的边界框坐标回归量每个提出边界框b∈Bf和产生更新b∗估计的建议。

       8. 一个边界框由它的左上角和右下角参数化，使得边界框b = (x1, y1, x2, y2)。

       9. 完整的图像I被裁剪成以区域b为中心的矩形，宽度和高度通过比例因子膨胀

       10. 由此产生的图像是一个固定大小的W×H重新取样,Ib,这是由CNN处理回归的四值b *。

       11. 我们不直接返回边界框坐标的绝对值，而是返回编码的值。

       12. 左上坐标由Ib的左上象限编码，左下坐标由Ib的左下象限编码，如图3所示。

       13. 这将坐标标准化，使其一般落在区间[0,1]内，但允许在需要时打破该区间。

       14. 在实践中，我们将每个建议的裁剪面积扩大了两倍

       15. 这为CNN提供了足够的上下文来预测提案边界框的更准确位置。

       16. CNN使用(Ib, bgt)的示例对进行训练，以将groundtruth包围框bgt从估计包围框b从I裁剪的子图像Ib中恢复出来。

       17. 这是通过最小化编码边界框之间的L2损失来实现的，即
           $$
           min_{\Phi} \sum_{b\_belong\_B_train}|| g(I_b; \Phi) - q(b_{gt})||^2_2
           $$

       18. 通过cnn 在训练集 $B_{train}$上训练参数Φ，其中g是CNN前向传递函数和q是边界框坐标编码器

       19. 通过对多种不同的选择进行实验，选择了哪些特征以及使用的分类器和回归方法

       20. 这包括使用CNN进行分类，使用专用的CNN进行分类，以及联合训练单个CNN同时进行分类和回归，并进行多任务学习

       21. 然而，CNN的分类性能并没有明显优于随机森林的HOG，但是需要更多的计算和GPU来处理

       22. 因此，我们选择随机森林分类器来减少管道的计算成本，而不影响最终结果。

       23. 包围框回归不仅改善了重叠，以帮助对每个单独的样本进行文本识别，而且还使许多建议集中在同一个单词实例的包围框坐标上，从而帮助8.3节中描述的具有重复检测的投票/合并机制

    ### Text Recognition

    1. 在我们处理管道的这个阶段，已经生成了一组准确的单词边界框建议，如前面几节所述

    2. 现在我们开始识别这些建议的边界框中的单词。

    3. 为此，我们使用深度CNN在预先定义的单词字典-字典编码-显式建模自然语言的字典中执行分类

    4. 将每个边界框的裁剪图像作为输入输入CNN, CNN生成字典中所有单词的概率分布

    5. 以概率最大的单词作为识别结果

    6. 在6.2节中完整描述的模型可以扩展到包含90k个单词的大型词典，包括大多数常用英语(参见8.1节了解所使用的词典的详细信息)。

    7. 然而，要实现这一点，必须收集每个可能单词的许多训练样本

    8. 这样的训练数据集是不存在的，所以我们使用第6.1节中描述的合成训练数据来训练CNN。

    9. 这种合成的数据是如此真实，以至于CNN可以只根据合成的数据进行训练，但仍然适用于真实世界的数据。

       #### Synthetic Training Data

       1. 本节描述我们的场景文本渲染算法

       2. 由于我们的CNN模型将整个单词图像作为输入，而不是单个字符图像，因此访问覆盖整个语言或至少一个目标词汇的裁剪单词图像训练数据集是非常必要的

       3. 虽然有一些来自ICDAR的公开数据集(Karatzas et al. 2013;卢卡斯2005;Lucas et al. 2003;Shahab et al. 2011)、Street View Text (SVT)数据集(Wang et al. 2011)、IIIT-5k数据集(Mishra et al. 2012)等，全词图像样本数量仅为数千个，词汇量非常有限。

       4. 由于缺乏完整的单词图像样本，导致以前的工作转而依赖字符分类器(因为字符数据丰富)，或者通过挖掘数据或访问大型专有数据集来缓解这种训练数据的不足(Bissacco et al. 2013;Goodfellow等2013;Jaderberg et al. 2014)。

       5. 然而，我们希望执行基于整个单词图像的识别，而不是字符识别，并以一种可扩展的方式来实现这一点，而不需要人工标记的数据集。

       6. 在一些合成字符数据集取得成功后(de Campos et al. 2009; Wang et al. 2012)，我们创建了一个合成字数据生成器，能够模拟场景文本图像的分布

       7. 这是一个合理的目标，因为在自然场景中发现的大部分文本都被限制在一组有限的计算机生成的字体中，只有物理渲染过程(例如打印、绘画)和成像过程(例如相机、视点、光照、杂乱)不受计算机算法的控制

       8. 图4展示了生成过程和一些合成数据样本。

       9. 这些示例由三个独立的图像层(背景图像层、前景图像层和可选的边框/阴影图像层)组成，它们以带有alpha通道的图像的形式存在。

       10. 合成数据生成过程如下：
           1. **Font rendering**：从从谷歌字体下载的1400多种字体目录中随机选择一种字体。
           2. 字距、权重、下划线和其他属性是从任意定义的分布中随机变化的
           3. 这个单词被渲染到前景图像层的alpha通道上，要么使用水平的底部文本行，要么使用随机曲线。
           4. **Border/shadow rendering**：可以从前景呈现具有随机宽度的嵌入边框、开始边框或阴影。
           5. **Base colouring**：这三个图像层的每一层都填充了从自然图像上的簇中采样的不同的统一颜色。
           6. 这些聚类是通过k-means将Lucas等(2003)的训练数据集中每幅图像的RGB分量聚类成三个聚类。
           7. **Projective distortion**：前景和边缘/阴影图像层被随机的、完整的投影变换扭曲，模拟三维世界。
           8. **Natural data blending**：每个图像层都与ICDAR 2003和SVT的训练数据集随机采样的图像作物混合
           9. 混合和alpha混合模式的数量(例如普通，添加，复制，刻蚀，最大等等)是由一个随机的过程决定的，这创造了一个折衷的纹理和合成范围
           10. 这三个图像层也以一种随机的方式混合在一起，以得到一个单一的输出图像。
           11. **Noise**：类似于Simard等人(2003)的弹性失真，将高斯噪声、模糊、重采样噪声和JPEG压缩构件引入到图像中。
           12. 这个过程从大量的随机分布中抽取大量的合成数据样本，模拟真实世界的场景文本图像样本。
           13. 合成数据用于替代真实数据，标签根据需要从语料库或词典生成。
           14. 通过创建比现有数据集大许多数量级的训练数据集，我们能够使用渴求数据的深度学习算法来训练更丰富的、基于单词的模型。

           #### CNN Model

           1. 本节描述我们的单词识别模型

           2. 我们将识别问题表述为一个多类分类问题，每类一个单词，其中的单词被限制在预先定义的字典W中进行选择

           3. 虽然自然语言的字典W似乎太大，这种方法不可行，但在实践中，一个高级英语词汇，包括不同的单词形式，只包含大约90k个单词，这是大的，但易于管理。

           4. 详细,我们建议使用一个CNN的分类器,每个单词w∈W词汇对应一个输出神经元

           5. 我们使用具有5个卷积层和3个完全连接层的CNN，具体细节见第8.2节。

           6. 最后一个完全连接的层在单词字典中执行分类，因此具有与我们希望识别的字典大小相同的单元数。

           7. 预测词识别结果w∗的所有字典单词的集合w的语言L对于一个给定的输入图像x是给定的
              $$
              w^* = argmax_{w-belong-W}p(w|x, \Gamma)
              $$

           ​          $P(w|x,\Gamma)​$可以写作：

$$
          P(w|x,\Gamma) = {P（w|x）P(Pw|\Gamma)P(x)）}/{p(x|\Gamma)P(w)}
$$
​          		假设x不依赖于$\Gamma$在我们学习任何语言之前所有单词都是等概率的，我们的得分函数可以简化为
$$
          w* = argmax_{w-belong-W}P(w|x)P(w|\Gamma)
$$

​			8. 每个单词的输出概率P(w|x)由识别CNN的最后一个完全连接层的softmax输出来建模，基于语言			的单词先验$P(w|\Gamma)​$可以通过词汇或频率计数来建模

1. 网络示意图如图5所示。
2. N模型的一个限制是输入x必须是一个固定的、预定义的大小
3. 这对于单词图像来说是有问题的，因为尽管图像的高度总是一个字符高，但是单词图像的宽度高度依赖于单词中的字符数，字符数的范围可以在1到23个字符之间
4. 为了解决这个问题，我们只需将单词图像重新采样到一个固定的宽度和高度。
5. 虽然这并不保留纵横比，但是图像特征的水平频率失真很可能为网络提供了单词长度提示
6. 我们也尝试了不同的填充方式来保持长宽比，但是结果并不像单纯的重采样那么好。
7. 总结一下，对于图片I上，每个建议的boundin box b belong to Bf，我们计算$P(w|x_b,\Gamma)$通过裁剪图像到$I_b = c(b,I)$,重采样到一个固定的维度WxH,比如$x_b = R(I_b, W,H)$,并且计算$p(w|x_b)$通过计算文字识别CNN和乘以$P(w|\Gamma)$ (任务依赖)来给一个最后的概率分布在单词$P(w|x_b,\Gamma)​$上

### Merging & Ranking

1. 在这一点上,我们有一组词为每个图像边界盒B与它们相关的词概率分布$P_{Bf} = \{P_b: b ∈B_f\}​$, $P_b = P (w | b,I)= P (w | x_b, \Gamma)​$。

2. 然而，这组检测仍然包含许多单词的假阳性和重复检测，因此必须根据手头的任务(文本定位或基于文本的图像检索)对检测进行最终的合并和排序。

   #### Text Spotting

   1. 文本定位的目标是定位和识别图像中的单个单词

   2. 每个单词都应该用一个包围单词的边框来标记，边框应该有一个相关的文本标签。

   3. 对于这个任务,我们分配每一个边界框在$b ∈b_f​$, 标签$w_b​$和分数$S_b​$根据b的最大单词概率:
      $$
      w_b = argmax_{w∈W}P(w|b,I), \quad s_b = max_{w∈W}P(w|b,I)
      $$

   4. 为了对同一个单词实例的重复检测进行聚类，我们对具有相同单词标签的检测进行贪婪的非最大抑制(NMS)，将被抑制的建议的分数进行聚合

      这可以看作是对特定单词的位置投票

   5. 然后，我们使用NMS来抑制具有重叠的不同单词的非最大检测

      我们的文本识别CNN能够准确识别非常松散裁剪的单词子图像中的文本

      6. 因此，我们发现一些有效的文本定位结果与groundtruth的重叠小于0.5，但是对于某些应用程序，我们需要大于0.5的重叠

   7. 为了提高检测结果的重叠性，我们还像上文第5.2节和NMS中所描述的那样，进行多轮的包围盒回归，进一步细化我们的检测。

   8. 这可以看作是一个recurrent regressor network.网络。

     9. 每一轮回归都更新对每个单词本地化的预测，为下一轮回归提供一个更新的上下文窗口来执行下一次回归，如图6所示。

     10. 在每个回归之间执行NMS会导致在最近一轮回归之后变得相似的边界框被分组为单个检测

     11. 这通常会使检测的重叠部分收敛到一个更高的、稳定的值，只需要进行几轮回归。

     12. 元组(b、wb、sb)给出的精炼结果根据其得分sb进行排序，阈值决定最终的文本定位结果。

     13. 为了直接比较各个图像的得分，我们将每个图像的结果的得分按该图像中检测的最大分数进行标准化。

       #### Image Retrieval
    
       1. 对于基于文本的图像检索任务，我们希望检索包含给定查询词的图像列表。
    
       2. 查询词的本地化不是必需的，只是为检索图像提供证据时可选的。
    
       3. 这是通过在查询时，对每个图像I为每个查询单词Q{q1,q2,...}分配一个分数$s_I^Q$,并且对数据库$\Gamma$中的图像进行分数的降序排序
    
       4. 它还要求所有图像的分数可以快速计算，以扩展到数百万图像的数据库，允许通过文本搜索快速检索视觉内容。
    
       5. 虽然检索通常只针对单个查询词(Q = {q})执行，但是我们将检索框架泛化为能够处理多个查询词。
    
       6. 我们通过平均图像中所有探测到的bf的单词概率分布来估计单词空间P(w|I)的每幅图像的概率分布
             $$
             p_I = P(w|I) = \frac {1}{|B_f|} \sum_{b∈B_f}P_b
             $$
    
       7. 这个分布对于所有$I∈\Gamma$离线计算的
    
       8. 在查询时，我们可以简单地为每个图像$S_I^Q$计算一个分数，该分数表示图像I包含任何查询词Q的概率。
    
       9. 假设查询词之间存在独立性:
             $$
             s_I^Q = \sum_{q∈Q}P(q|I) = \sum_{q∈Q}p_I(q)
             $$
             其中$p_I(q)$是在单词分布$p_I$中查找单词q的概率。
    
          10. 这些分数可以非常快速、高效地计算通过构造$p_I∀I∈\Gamma$的反向索引。
    
         11. 在对pI进行一次性脱机预处理并组装反向索引之后，可以在不到一秒的时间内跨数百万张图像的数据库处理查询

   ### Experiments

   1. 在本节中，我们将根据一些标准文本定位和基于文本的图像检索基准来评估管道。

   2. 我们介绍了8.1节中用于评估的各种数据集，在8.2节中给出了我们管道中每个部分的准确实现细节和结果，最后分别介绍了8.3节和8.4节中关于文本定位和图像检索基准的结果。

      #### Datasets

      1. 我们在大量的数据集上评估我们的管道
      2. 由于注释的级别不同，数据集用于文本识别、文本定位和图像检索评估的组合。
      3. 表1、表2和表3总结了这些数据集。
      4. 一些数据集提供的更小的词汇表用于将搜索空间减少到词汇表中包含的文本。
      5. Synth数据集是由我们6.1节的合成数据引擎生成的。
      6. 我们产生900万32×100图片,相同数量的词样本90 k字典
      7. 我们将其中900k用于测试数据集，900k用于验证，其余用于培训
      8. 90k词典由Hunspell (http://hunspell.sourceforge.net/)提供的英语词典组成，Hunspell是一个流行的开源拼写检查系统。
      9. 该字典由50k个根单词组成，我们将其展开，以包括所有可能的前缀和后缀，以及添加来自ICDAR、SVT和IIIT数据集的测试数据集单词——总共90k个单词。
      10. 该数据集可在http://www.robots.ox.ac.uk/~vgg/data/text/上公开获取。
      11. ICDAR 2003 (IC03) (http://algoval.essex.ac.uk/icdar/datasets.html)、ICDAR 2011 (IC11) (Shahab et al. 2011)和ICDAR 2013 (IC13) (Karatzas et al. 2013)是场景文本识别数据集，分别由251、255和233幅完整的场景图像组成
      12. 照片由一系列场景组成，并提供了文字级注释
      13. 三个数据集之间的大部分测试数据是相同的
      14. 对于IC03, Wang等人(2011)定义了每幅图像50个单词词汇(IC03-50)和所有测试groundtruth单词的词汇(IC03- full)。
      15. 对于IC11, Mishra等人(2013)定义了一个包含538个查询词的列表，用于评估基于文本的图像检索。
      16. StreetView Text (SVT) data etwang et al.(2011)包括从谷歌StreetView下载的249张路边场景的高分辨率图像。
      17. 这是一个具有挑战性的数据集，有很多噪音，还有很多没有注释的单词。
      18. 每个图像还提供了50个单词词汇表(SVT-50)。
      19. IIIT 5k-word数据集Mishra等(2012)包含了从谷歌图像搜索中获得的场景文本和数字图像的3000个裁剪词图像
      20. 这是目前最大的自然图像文本识别数据集。
      21. 每个单词图像都有一个关联的50个单词词典(IIIT5k-50)和1k个单词词典(IIIT5k-1k)。
      22. IIIT场景文本检索(STR) Mishra等(2013)是一种基于文本的图像检索数据集，也是通过谷歌图像搜索收集的
      23. 这50个查询词中的每一个都有一个包含查询词的10-50个图像的关联列表
      24. 还有大量没有从Flickr下载文本的干扰图片。
      25. 总共有10k张图片，没有提供单词边界框注释
      26. IIIT sports -10k数据集Mishra等(2013)是基于运动视频帧构建的另一种基于文本的图像检索数据集
      27. 这些图像的分辨率很低，而且经常有噪声或模糊，文本通常位于广告和招牌上，这使得这是一个具有挑战性的检索任务。
      28. 10个查询词共提供10k个图像，不带单词边框注释
      29. BBC新闻是英国广播公司(BBC)在2007年至2012年间播出的节目框架的专有数据集。
      30. 大约5000 h的视频(大约1200万帧)处理选择230万关键帧在1024×768分辨率
      31. 这些视频取自一系列不同的BBC新闻和时事节目，包括BBC晚间新闻节目
      32. 文本通常出现在框架中，来自人工插入的标签、字幕、新闻标记文本和一般场景文本。
      33. 此数据集不提供标签或注释。

      #### Implementation Details

      1. 我们为管道中的每个阶段训练一个单独的模型，并使用ICDAR和SVT的训练数据集选择超参数

      2. 所有数据集和实验都使用完全相同的管道，具有相同的模型和超参数

      3. 这突出了端到端框架对不同数据集和任务的通用性

      4. 随着管道的推进，检测召回的进度和建议的数量如图7所示。

         ##### Edge Boxes & ACF Detector

         1. 边缘盒检测器具有多个超参数，控制评价步长和非最大抑制。
         2. 我们使用的默认值α= 0.65,β= 0.75(有关详细信息,请参阅Zitnick和Dollar2014的参数)。
         3. 在实践中，我们发现改变这些参数在联合recall中效果甚微。
         4. 对于ACF检测器，我们将每一轮引导的决策树数量设置为32,128,512
         5. 对于feature聚合,我们使用4×4块与[121]/ 4滤波器平滑,每倍频程8级。
         6. 当探测器被训练成一个特定的纵横比时，我们在[1,1.2,1.4，…]范围内进行多个纵横比的检测。， 3]以解释可变大小的单词
         7. 我们训练30 k剪裁32×100积极词合并样本中从大量的训练数据集Jaderberg et al .(2014),和从11 k随机负样本-补丁图像不包含文本。
         8. 图8显示了提案生成阶段的性能。
         9. 在IC03和SVT数据集中，groundtruth标记的单词以0.5个重叠的频率被召回，这是每个图像生成的建议区域数量的函数
         10. Edge Boxes的最大召回率为92%，ACF检测器的最大召回率约为70%。
         11. 然而，结合每种方法的建议，IC03和SVT的召回率分别为6k建议的98%和11k建议的97%
         12. 特定提案与groundtruth边界框的平均最大重叠在IC03上为0.82，在SVT上为0.77，这表明区域提案技术能够在数千个误报中产生一些准确的检测
         13. 这种高召回率和高重叠率为我们的后续工作提供了一个良好的起点，并极大地减少了单词检测的搜索空间，从数千万个可能的边界框减少到每个图像大约10k个建议。

         #### Random Forest Word Classifier

         1. 随机森林字/无字二元分类器作用于裁剪区域的建议。
         2. 这些重新取样32×100固定大小,和HOG特征提取与cell大小为4,结果产生hog向量 $h∈R^{8x25x36}$ ,7200维的描述符。
         3. 随机森林分类器由10棵树组成，最大深度64
         4. 对于训练，我们在ICDAR和SVT的训练数据集的第4节中描述，提取区域建议，定义正边界盒样本与groundtruth的重叠至少0.5，负边界盒样本与groundtruth的重叠小于0.3
         5. 由于阴性样本数量较多，我们随机抽取相同数量的阴性样本与阳性样本，分别给予300k阳性和400k阴性训练样本。
         6. 一旦训练，结果是一个非常有效的假阳性过滤器。
         7. 我们选择0.5的操作概率阈值，对IC03和SVT阳性建议区域分别给予96.6和94.8%的召回
         8. 这种过滤将区域建议的总数平均减少到每张图像650个(IC03)和900个(SVT)建议。

         #### Bounding Box Regressor

         1. bounding box回归CNN由四个卷积层，每一层的输入分别为（过滤器大小，过滤器数量），{5，64}，{5，128}，{3，256}，{3，512}，然后是两个完全连接的层，其中4k单元和4个单元(每个回归变量一个)
         2. 所有隐藏层之后是修正的线性非线性，卷积层的输入是零以保持维数，卷积层之后用2x2最大池化
         3. CNN的固定大小的输入是一个32×100灰度级图像，通过减去图像均值集中在零点附近，并通过除以标准差均值标准化。
         4. 对CNN进行全连通层的随机梯度下降(SGD)和dropout (Hinton et al. 2012)训练，减少过拟合，最小化估计值与groundtruth边界框之间的L2距离(Eq. 1)。
         5. 我们使用了700k的训练例子，其中边界框提案与在ICDAR和SVT训练数据集上计算的groundtruth重叠大于0.5。
         6. 回归前，平均正建议区域(与groundtruth重叠超过0.5)在IC03和SVT上的重叠为0.61和0.60。
         7. CNN将IC03和SVT的平均正重叠提高到0.88和0.70。

         #### Text Recognition CNN

         1. 文本识别CNN由8个权重层组成——5个卷积层和3个完全连接的层。
         2. 卷积层如下（过滤器大小，过滤器数量），{5，64}，{5，128}，{3，256}，{3，512}
         3. 前两个完全连接的层有4k单位，最后一个完全连接的层的单位数量与字典中的单词数量相同——在我们的示例中是90k单词。
         4. 最后的分类层是一个softmax标准化层。
         5. 修正线性非线性跟踪每一个隐藏层,但除了第四卷积层跟在2×2 max pooling后面
         6. 卷积层的输入是零填充的，以保持维数
         7. CNN的固定大小的输入是一个32×100灰度级图像，通过减去图像均值集中在零点附近，并通过除以标准差均值标准化。
         8. 我们在Synth训练数据上训练网络，反向传播标准多项式逻辑回归损失。
         9. 优化使用SGD与dropout调整的全连接层，我们动态地降低学习率随着train的进展
         10. 通过对训练数据中类的均匀采样，我们发现SGD批大小必须至少为总类数的五分之一才能进行网络训练。
         11. 对于非常多的class(即超过5k的class)，有效训练所需的SGD批大小变得很大，大大降低了训练速度。
         12. 因此，对于大型字典，我们执行增量式培训，以避免需要非常大的批处理大小。
         13. 这包括最初用5k类训练网络，直到部分收敛，然后再添加额外的5k类。
         14. 原始的权值被复制到之前训练过的类中，额外的分类层权值被随机初始化
         15. 然后，该网络可以继续训练，额外的随机初始化权重和类会导致训练错误激增，而这些错误很快就会被消除
         16. 在添加更多的类之前，允许对类的子集进行部分收敛的过程会重复进行，直到达到所需的类的全部数量。
         17. 在评估时，我们不做任何数据扩充。
         18. 如果提供了词汇，我们将语言先验$P(w|\Gamma)$设置为词汇单词的概率相等，否则为零
         19. 在没有词典的情况下，P(w|L)计算为语料库(我们使用opensubtitles.org英语语料库)中单词w的频率，并进行幂律规范化
         20. 该模型总共包含大约5亿个参数，可以在带有Caffe自定义版本的GPU上以2.2ms的速度处理一个单词
         21. 识别结果我们在大范围的数据集和词汇表大小上评估文本识别模型的准确性。
         22. 我们遵循Wang et al.(2011)的标准评价方案，对仅包含字母数字字符和至少三个字符的单词进行识别。
         23. 结果如表4所示，突出了我们deep CNN的出色表现
         24. 虽然我们只训练纯合成数据，没有人工注释，但是我们的模型在所有标准数据集的最新准确性方面取得了显著的改进
         25. 在IC03-50上，识别问题得到了很大程度上的解决，准确率达到了98.7%——860个测试样本中只有11个错误——而且我们在SVT-50上显著优于先前的先进技术(Bissacco et al. 2013)，准确率提高了5%，IC13提高了3%。
         26. 与ICDAR数据集相比，在不受特定于数据集的词典约束的情况下，SVT的准确率更低，为80.7%。
         27. 这反映了SVT数据集的困难，因为图像样本可能是非常低质量、噪声和低对比度的
         28. Synth数据集的精确性表明，我们的模型确实能够在整个90k字典中一致地识别单词样本。
         29. **Synthetic Data Effects** 作为一个额外的实验，我们将研究第6.1节中合成数据生成引擎的各个阶段对实际识别精度的贡献
         30. 我们定义了两个简化的识别模型(为了计算速度)，字典只包含IC03和SVT完整词典，分别表示为DICT-IC03-Full和DICT-SVT-Full，它们只在各自的数据集上测试。
         31. 我们使用相同的训练过程，但是随着合成数据的复杂性的增加，重复地从头开始训练这些模型
         32. 图9显示了随着使用更复杂的合成训练数据，这些模型的测试精度是如何提高的
         33. 随机图像层着色的添加会显著提高性能(IC03 +44%， SVT +40%)，自然图像混合的添加也会显著提高性能(IC03 +1%， SVT +6%)。
         34. 有趣的是，与IC03数据集相比，通过在SVT数据集上合并自然图像混合，可以观察到准确性有了更大的提高。
         35. 这很可能是由于与IC03相比，SVT中的文本有更多不同和复杂的背景。

      #### Text Spotting

      1. 在文本识别任务中，目标是定位和识别测试图像中的单词
      2. 除非另有说明，我们遵循Wang et al.(2011)的标准评估协议，忽略所有包含字母数字字符且长度不超过三个字符的单词
      3. 只有当检测边界框与groundtruth至少有0.5个重叠(IoU)时，识别结果才有效
      4. 表5显示了与以前的方法相比，我们的文本定位管道的结果
      5. 我们报告数据集中所有图像的全局F-measure。
      6. 在所有数据集中，我们的管道的性能大大优于以前的所有方法
      7. 在SVT-50上，我们将最先进的技术提高了20%，达到了0.85/0.68/0.76的P/R/F (precision/recall/F-measure)，而Jaderberg et al.(2014)中为0.73/0.45/0.56。
      8. 从图10的查全率/查全率曲线可以看出，我们的流水线能够维持很高的查全率，而我们的文本识别系统的识别分数对于检测的适用性有很强的提示作用。
      9. 当没有词汇表时，我们也会给出所有数据集的结果。
      10. 正如预期的那样，F-measure缺乏词汇约束，但仍然显著高于其他类似的工作。
      11. 应该注意的是，SVT数据集仅部分注释
      12. 这意味着如果完全注释，那么精确度(以及F-measure)要比真正的精度低得多，因为检测到的许多单词没有注释，因此被记录为假阳性。
      13. 但是，我们可以报告SVT-50的召回率为71,SVT为59%。
      14. 有趣的是，当重叠阈值降低到0.3时(表5的最后一行)，我们看到ICDAR数据集有了小的改进，SVT-50有了大的+8%的改进。
      15. 这意味着我们的文本识别CNN能够准确识别即使是松散剪裁的检测
      16. 忽略正确识别单词的要求，即在执行纯单词检测时，IC03和IC11的f值分别为0.85和0.81。
      17. 一些示例文本定位结果如图11所示
      18. 由于我们的管道不依赖于基于连接组件的算法或显式字符识别，我们可以检测和识别分离、遮挡和模糊的单词
      19. 我们系统的一个常见**故障模式**是由于缺少合适的建议区域而导致单词的丢失，特别是对于**倾斜或垂直文本**，这一点在我们的框架中没有明确建模
      20. 此外，检测子词或多个词在一起也会导致假阳性结果。

      #### Image Retrieval

      1. 我们还将管道应用于基于文本的图像检索任务。
      2. 给定一个文本查询，必须返回包含查询文本的图像。
      3. 该任务使用Mishra et al.(2013)的框架进行评估，结果如表6所示。
      4. 对于每个已定义的查询，我们检索数据集所有图像的排序列表，并计算每个查询的平均精度(AP)，报告所有查询的平均平均精度(mAP)。
      5. 在所有数据集上，我们的表现都明显优于Mishra等人——我们在IC11上获得了一张90.3%的地图，而Mishra等人(2013)获得的地图是65.3%。
      6. 我们的方法无缝扩展到更大的体育数据集，其中我们的系统在20幅图像(P@20)上达到了92.5%的精度，是Mishra et al.(2013)的43.4%的两倍多。
      7. Mishra et al.(2013)还报告了其他文本定位算法的已发布实现的SVT检索结果
      8. Wang et al.(2011)的方法实现了21.3%的mAP, Neumann和Matas(2012)的方法实现了23.3%的mAP, Mishra et al.(2013)的方法实现了56.2%的mAP，而我们自己的结果是86.3%的mAP。
      9. 然而，与SVT的文本定位结果一样，我们的检索结果在SVT和Sports data - fig上注释不完整。显示了这个问题是如何影响精度的。
      10. 结果是SVT上的真实映射比报告的86.3%的映射要高。
      11. 根据图像分辨率的不同，我们的算法需要大约5-20秒来计算每张图像的端到端结果在单个CPU核心和单个GPU上。
      12. 我们分析每个阶段所花费的时间的管道在SVT的数据集,而平均图像大小为1260×860,结果显示在表7所示。
      13. 由于我们减少了整个管道中的提案数量，我们可以允许每个提案的处理时间增加，同时保持每个阶段的总处理时间稳定。
      14. 随着管道的发展，我们可以使用更复杂的计算特性和分类器。
      15. 我们的方法可以简单地并行化，这意味着我们可以在一个拥有16个物理CPU内核和4个普通gpu的高性能工作站上每秒处理1-2张图像。
      16. 管道的高精度和高速度使我们能够处理大量的数据集，用于实际的搜索应用
      17. 我们在5000小时的BBC新闻数据集中演示了这一点。
      18. 围绕我们的图像检索管道构建一个搜索引擎和前端web应用程序，允许用户立即搜索大量视频数据集中出现的可视文本
      19. 这非常有效，图13显示了一些来自我们的可视化搜索引擎的示例检索结果
      20. 虽然我们没有groundtruth注释来量化该数据集上的检索性能，但是我们测量了图13中测试查询的精确度为100 (P@100)，其中hollywood和boris johnson查询的P@100为100%，vision为93%
      21. 这些结果展示了我们的框架的可伸缩性

      ### Conclusions

      1. 在本文中，我们提出了一种端到端文本读取流水线——一种自然场景图像中文本的检测与识别系统
      2. 这个通用系统在文本定位和图像检索任务中都运行得非常好，与以前的所有方法相比，在所有标准数据集上，在没有任何特定于数据集的调优的情况下，显著提高了这两个任务的性能。
      3. 这在很大程度上归因于一个非常高的召回建议阶段和一个文本识别模型，该模型比以前的所有系统都具有更高的准确性。
      4. 我们的系统是快速和可扩展的——我们演示了从数百幅图像的数据集到能够处理数百万幅图像的数据集进行基于文本的即时图像检索的无缝可伸缩性，而不会降低任何可感知的准确性。
      5. 此外，我们的识别模型能够完全基于合成数据进行训练，这使得我们的系统能够很容易地进行再训练，以识别其他语言或脚本，而无需任何人工标记。
      6. 我们为文本定位和图像检索设置了一个新的基准。
      7. 展望未来，我们希望探索更多的识别模型，以实现对未知单词和任意字符串的识别。


​             

